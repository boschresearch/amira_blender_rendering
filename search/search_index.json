{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AMIRA Blender Rendering (ABR)\u2019s documentation! \u00b6 ABR is a pipeline for rendering large photo-realistic datasets, and is built around blender . By using blender to model scenes, we allow users to address their specific requirements with respect to scenarios and model properties with capabilities of a major and free 3D creation suite. Afterwards, ABR can be used to handle generation of large amounts of photo-realistic data, for intance to train deep networks for object recognition, segmentation, pose estimation, etc. Note that, currently, ABR is operated mostly from the command line. The primary reason for this is that ABR\u2019s intended purpose is also to be used on headless GPU clusters or rendering farms. Workflow \u00b6 The workflow of using ABR is held as simple as possible. First, in blender , you either develop your own scenario for which you would like to generate a dataset or adapt one of the existing scenarios. If you decided on your own scenario, or if you heavily modified an existing scneario, you might need to provide what we call a scenario backend which will take care of setting up the file. For instance, you might wish to randomize object locations in every rendered image. Next, you need to specify rendering information such as camera calibration data or the number of images you would like to obtain in a Configuration ). Finally, you commence dataset generation by running the provided abrgen command. As outlined above, each scene requires what we call a backend implementation . This implementation takes care of loading a blender file, setting up everything that is required such as camera information, objects, randomization, etc. It also contains a main loop which does the rendering for the number of desired images. An exemplary backend implementation can be found in ABR source tree at src/amira_blender_rendering/scenes/workstationscenarios.py . This backend implementation reads all optional configuration parameters either from a configuration file that is passed along to the rendering script, or from the additional parameters passed during execution. An example for a configuration that contains documentation for all options can be found in ABR source tree at config/examples/workstation_scenario01_test.cfg . Note that configuration options depend on the specified blender scene and backend implementation. Find out more about how to use ABR and Configuration . Citing ABR \u00b6 If you use ABR please make sure to cite our work. @misc { amira _ blender _ rendering _ 2020, author= { N.Waniek, M.Todescato, M.Spies, M.Buerger } , title= { AMIRA Blender Rendering } , year= { 2020 } , url= { https://github.com/boschresearch/amira-blender-rendering } , } Contacts \u00b6 Nicolai Waniek Marco Todescato Markus Spies","title":"Overview"},{"location":"#welcome-to-amira-blender-rendering-abrs-documentation","text":"ABR is a pipeline for rendering large photo-realistic datasets, and is built around blender . By using blender to model scenes, we allow users to address their specific requirements with respect to scenarios and model properties with capabilities of a major and free 3D creation suite. Afterwards, ABR can be used to handle generation of large amounts of photo-realistic data, for intance to train deep networks for object recognition, segmentation, pose estimation, etc. Note that, currently, ABR is operated mostly from the command line. The primary reason for this is that ABR\u2019s intended purpose is also to be used on headless GPU clusters or rendering farms.","title":"Welcome to AMIRA Blender Rendering (ABR)'s documentation!"},{"location":"#workflow","text":"The workflow of using ABR is held as simple as possible. First, in blender , you either develop your own scenario for which you would like to generate a dataset or adapt one of the existing scenarios. If you decided on your own scenario, or if you heavily modified an existing scneario, you might need to provide what we call a scenario backend which will take care of setting up the file. For instance, you might wish to randomize object locations in every rendered image. Next, you need to specify rendering information such as camera calibration data or the number of images you would like to obtain in a Configuration ). Finally, you commence dataset generation by running the provided abrgen command. As outlined above, each scene requires what we call a backend implementation . This implementation takes care of loading a blender file, setting up everything that is required such as camera information, objects, randomization, etc. It also contains a main loop which does the rendering for the number of desired images. An exemplary backend implementation can be found in ABR source tree at src/amira_blender_rendering/scenes/workstationscenarios.py . This backend implementation reads all optional configuration parameters either from a configuration file that is passed along to the rendering script, or from the additional parameters passed during execution. An example for a configuration that contains documentation for all options can be found in ABR source tree at config/examples/workstation_scenario01_test.cfg . Note that configuration options depend on the specified blender scene and backend implementation. Find out more about how to use ABR and Configuration .","title":"Workflow"},{"location":"#citing-abr","text":"If you use ABR please make sure to cite our work. @misc { amira _ blender _ rendering _ 2020, author= { N.Waniek, M.Todescato, M.Spies, M.Buerger } , title= { AMIRA Blender Rendering } , year= { 2020 } , url= { https://github.com/boschresearch/amira-blender-rendering } , }","title":"Citing ABR"},{"location":"#contacts","text":"Nicolai Waniek Marco Todescato Markus Spies","title":"Contacts"},{"location":"contributing/","text":"Contributing to AMIRA Blender Rendering \u00b6 Want to contribute? That\u2019s great! Any contribution is welcome! You can do that by: Submitting Issues in the GitHub issue tracking system Contributing Code using the standard GitHub pull request model For large contributions we do encourage you to file a ticket in the GitHub issue tracking system prior to any development to coordinate with the ABR development team early in the process. Coordinating up front helps to avoid frustration later on. Also, in case you need help and/or support you can always contact us. Finally, you can also fork and work on your own but we discourage this operating mode unless rather than contributing you aim at developing your own cool project based on ABR. For additional information about how to contribute and the legal requirements, please refer to the CONTRIBUTING.md notice file located at the root directory of the project repo. NOTE : According to AMIRA Blender Rendering license clause, your contribution must be licensed as Apache-2.0. Submitting Issues \u00b6 Contact us and/or open an issue in the project Issue Tracker. While doing so please provide: a meaningful title; a meaningful (concise and clear) description with the minimum amount of necessary info either to replicate the issue or to describe what the requested should provide. Contributing Code \u00b6 In this case we recommend to: Contact us Open an Issue (as described above) Open a PR from you feature/branch against the branch you want to contribute (e.g., master or develop)","title":"Contributing"},{"location":"contributing/#contributing-to-amira-blender-rendering","text":"Want to contribute? That\u2019s great! Any contribution is welcome! You can do that by: Submitting Issues in the GitHub issue tracking system Contributing Code using the standard GitHub pull request model For large contributions we do encourage you to file a ticket in the GitHub issue tracking system prior to any development to coordinate with the ABR development team early in the process. Coordinating up front helps to avoid frustration later on. Also, in case you need help and/or support you can always contact us. Finally, you can also fork and work on your own but we discourage this operating mode unless rather than contributing you aim at developing your own cool project based on ABR. For additional information about how to contribute and the legal requirements, please refer to the CONTRIBUTING.md notice file located at the root directory of the project repo. NOTE : According to AMIRA Blender Rendering license clause, your contribution must be licensed as Apache-2.0.","title":"Contributing to AMIRA Blender Rendering"},{"location":"contributing/#submitting-issues","text":"Contact us and/or open an issue in the project Issue Tracker. While doing so please provide: a meaningful title; a meaningful (concise and clear) description with the minimum amount of necessary info either to replicate the issue or to describe what the requested should provide.","title":"Submitting Issues"},{"location":"contributing/#contributing-code","text":"In this case we recommend to: Contact us Open an Issue (as described above) Open a PR from you feature/branch against the branch you want to contribute (e.g., master or develop)","title":"Contributing Code"},{"location":"datasets/","text":"Datasets \u00b6 ABR\u2019s main purpose is to render datasets. If you rendered a dataset and made it publicly available, please drop us a note and a description of the dataset. We will then add it to the list of datasets and descriptions below. Currently we provide the following list of datasets: WIP","title":"Datasets"},{"location":"datasets/#datasets","text":"ABR\u2019s main purpose is to render datasets. If you rendered a dataset and made it publicly available, please drop us a note and a description of the dataset. We will then add it to the list of datasets and descriptions below. Currently we provide the following list of datasets: WIP","title":"Datasets"},{"location":"formats/","text":"Directory and File Format Specification \u00b6 Every dataset rendered using ABR follows the same folder and annotation structure. In the following we describe them both. Output Folder Structure \u00b6 Every dataset rendered, e.g., by running $ abrgen --config path/to/config.cfg follows the same folder structure. Assuming that [ dataset ] base_path = RootDir / BaseName [ scene_setup ] cameras = Camera # only frontal view (mono) camera it consists of the following RootDir/ \u2514\u2500\u2500 BaseName.Camera \u251c\u2500\u2500 Dataset.cfg: summary configuration for rendered dataset \u251c\u2500\u2500 Images/ | \u251c\u2500\u2500 backdrop/ : folder with background drop images (composite mask) | \u251c\u2500\u2500 range/ : folder with range (.exr) images | \u251c\u2500\u2500 depth/ : folder with depth (.png) images | \u251c\u2500\u2500 disparity/ : (if set in the config file) folder with disparity (.png) images | \u251c\u2500\u2500 masks/ : folder with mask for each segmented object | \u2514\u2500\u2500 rgb/ : folder with RGG images \u2514\u2500\u2500 Annotations/ \u251c\u2500\u2500 OpenCV/ : annotations with object poses in OpenCV convention \u2514\u2500\u2500 OpenGL/ : annotations with object poses in OpenGL convention Image indexing \u00b6 All image files (except for masks) are named based on the indexing scheme sXXX_vYYY where XXX and YYY are index determined depeding on the selected number of images and render-mode . For more info about the render-mode refer to render-modes . For you to know, in DEFAULT render mode XXX wgoes from 0 to N-1 where N is the total number of images in the dataset while YYY=0. In MULTIVIEW mode, XXX goes from 0 to S-1 where S is the selected number of scenes, while YYY goes from 0 to V-1 where V is the selected number of camera views. In this case the total number of images is SxV. Notice that, if necessary, XXX and YYY are zero-padded automatically. In order to distinguish different instances of objects as well as different object types, for masks we use the different convention sXXX_vYYY_t_i.png where: t : index for the object type i : instance number for the same object type Annotation File Contents \u00b6 Annotations are stored as .json files following the same naming convention used for images. That is to image n.png corresponds the annotation n.json, being n the image number. Each annotation file contains information about all the objects present in the scene in the form of a list of dictionaries where each dictionary refer to one object instance. Specifically, for each object we store: object_class_name (str): label/name for object type object_class_id (int): object type object_name (str): instance name object_id (int): instance number for object of same type mask_name (str): mask suffix visible (bool): visibility flag pose (dict): dictionary containing: q (list (4,)): object rotation w.r.t. the camera embedded as a quaternion (xyzw) t (list (3,)): object translation vector w.r.t. the camera bbox (dict): dictionary containing bounding boxes information: corners2d (list (2,2)): 2d bounding box corners in pixel space corners3d (list (9,2)): 3d bounding box corners in (sub)pixel space aabb (list (9,3)): axis aligned bounding box corners in 3D space oobb (list (9,3)): object oriented bounding box corners in 3D space camera_pose (dict): dictionary containing: q (list (4,)): camera rotation w.r.t. the world frame embedded as a quaternion (xyzw) t (list (3,)): camera translation vector w.r.t. the world frame ABR Datasets API (abr_dataset_tools) \u00b6 ABR ships also a standalone lean python package to handle datasets rendered using it. To access ABR\u2019s Datasets API, you need to install the abr_dataset_tools package in your working python environment by running (from the package root directory) (assuming pip3.7) ( active venv ) $ pip install . Then, to have a quick overview of how to use it, run ( active venv ) $ python -m abr_dataset_tools --help The package implements some basic functionalities to load/plot images and print information about a prescribed dataset.","title":"Formats"},{"location":"formats/#directory-and-file-format-specification","text":"Every dataset rendered using ABR follows the same folder and annotation structure. In the following we describe them both.","title":"Directory and File Format Specification"},{"location":"formats/#output-folder-structure","text":"Every dataset rendered, e.g., by running $ abrgen --config path/to/config.cfg follows the same folder structure. Assuming that [ dataset ] base_path = RootDir / BaseName [ scene_setup ] cameras = Camera # only frontal view (mono) camera it consists of the following RootDir/ \u2514\u2500\u2500 BaseName.Camera \u251c\u2500\u2500 Dataset.cfg: summary configuration for rendered dataset \u251c\u2500\u2500 Images/ | \u251c\u2500\u2500 backdrop/ : folder with background drop images (composite mask) | \u251c\u2500\u2500 range/ : folder with range (.exr) images | \u251c\u2500\u2500 depth/ : folder with depth (.png) images | \u251c\u2500\u2500 disparity/ : (if set in the config file) folder with disparity (.png) images | \u251c\u2500\u2500 masks/ : folder with mask for each segmented object | \u2514\u2500\u2500 rgb/ : folder with RGG images \u2514\u2500\u2500 Annotations/ \u251c\u2500\u2500 OpenCV/ : annotations with object poses in OpenCV convention \u2514\u2500\u2500 OpenGL/ : annotations with object poses in OpenGL convention","title":"Output Folder Structure"},{"location":"formats/#image-indexing","text":"All image files (except for masks) are named based on the indexing scheme sXXX_vYYY where XXX and YYY are index determined depeding on the selected number of images and render-mode . For more info about the render-mode refer to render-modes . For you to know, in DEFAULT render mode XXX wgoes from 0 to N-1 where N is the total number of images in the dataset while YYY=0. In MULTIVIEW mode, XXX goes from 0 to S-1 where S is the selected number of scenes, while YYY goes from 0 to V-1 where V is the selected number of camera views. In this case the total number of images is SxV. Notice that, if necessary, XXX and YYY are zero-padded automatically. In order to distinguish different instances of objects as well as different object types, for masks we use the different convention sXXX_vYYY_t_i.png where: t : index for the object type i : instance number for the same object type","title":"Image indexing"},{"location":"formats/#annotation-file-contents","text":"Annotations are stored as .json files following the same naming convention used for images. That is to image n.png corresponds the annotation n.json, being n the image number. Each annotation file contains information about all the objects present in the scene in the form of a list of dictionaries where each dictionary refer to one object instance. Specifically, for each object we store: object_class_name (str): label/name for object type object_class_id (int): object type object_name (str): instance name object_id (int): instance number for object of same type mask_name (str): mask suffix visible (bool): visibility flag pose (dict): dictionary containing: q (list (4,)): object rotation w.r.t. the camera embedded as a quaternion (xyzw) t (list (3,)): object translation vector w.r.t. the camera bbox (dict): dictionary containing bounding boxes information: corners2d (list (2,2)): 2d bounding box corners in pixel space corners3d (list (9,2)): 3d bounding box corners in (sub)pixel space aabb (list (9,3)): axis aligned bounding box corners in 3D space oobb (list (9,3)): object oriented bounding box corners in 3D space camera_pose (dict): dictionary containing: q (list (4,)): camera rotation w.r.t. the world frame embedded as a quaternion (xyzw) t (list (3,)): camera translation vector w.r.t. the world frame","title":"Annotation File Contents"},{"location":"formats/#abr-datasets-api-abr_dataset_tools","text":"ABR ships also a standalone lean python package to handle datasets rendered using it. To access ABR\u2019s Datasets API, you need to install the abr_dataset_tools package in your working python environment by running (from the package root directory) (assuming pip3.7) ( active venv ) $ pip install . Then, to have a quick overview of how to use it, run ( active venv ) $ python -m abr_dataset_tools --help The package implements some basic functionalities to load/plot images and print information about a prescribed dataset.","title":"ABR Datasets API (abr_dataset_tools)"},{"location":"fqa/","text":"Frequently Questioned Answers \u00b6 Why is ABR a command line script, and not a blender GUI plugin? The intended usage of ABR is to 1) setup scenarios and models in blender, and 2) then run rendering of an entire dataset on a headless GPU cluster. Hence, the focus was primarily on writing the scripts that are required to automate the rendering process given a certain scenario. Note that we might add a blender GUI plugin at some point in the future. I really love ABR! Thanks, that\u2019s great to hear! If you see one of us at a conference, you might say thanks in person or buy us a beer or three ;-) I do not like ABR! Well, that\u2019s a bummer. Let us know what exactly you didn\u2019t like, so we can improve on it. If I need help, how can I get in contact with ABR\u2019s developers? You can reach out by shooting us an email. See our contacts","title":"FAQs"},{"location":"fqa/#frequently-questioned-answers","text":"Why is ABR a command line script, and not a blender GUI plugin? The intended usage of ABR is to 1) setup scenarios and models in blender, and 2) then run rendering of an entire dataset on a headless GPU cluster. Hence, the focus was primarily on writing the scripts that are required to automate the rendering process given a certain scenario. Note that we might add a blender GUI plugin at some point in the future. I really love ABR! Thanks, that\u2019s great to hear! If you see one of us at a conference, you might say thanks in person or buy us a beer or three ;-) I do not like ABR! Well, that\u2019s a bummer. Let us know what exactly you didn\u2019t like, so we can improve on it. If I need help, how can I get in contact with ABR\u2019s developers? You can reach out by shooting us an email. See our contacts","title":"Frequently Questioned Answers"},{"location":"installation/","text":"Installation \u00b6 This page explains how to install ABR. In order for ABR to properly interact with blender, it might be necessary to replace the python distribution shipped with Blender as below explained. We suggest to first try w/o doing so and resort to it if nothing else works. Tested requirements The following instructions have been tested for a variety of environments: OS: Ubuntu 18.04 (Bionic Beaver) and 20.04 (Focal Fossa) Blender: >=2.80, <=2.91.2 python3.7(.x) Using a more recent version of Blender should be possible but we do not ensure it. Installing ABR as a python package \u00b6 The easiest way to install (and later use) ABR can be done with pip . If you simply want to use ABR without adding your own scenes or without making modifications, the best way to install it so is to simply call pip install . from the root of ABR\u2019s source tree, i.e. the folder in which you can find the file setup.py . We call this the passive user mode . However, we expect users to be active . That is, we believe that, at some point, you might want to add your own scenes, make modifications to existing scenes, etc. If you are one of those active users , then you should rather install ABR in editable mode via pip install -e . from ABR\u2019s root folder. This way, you can still edit all files, add new scenes, without having to manually re-install ABR afterwards. There\u2019s also the possibility to use ABR without any installation procedure. More about this can be found in using . NOTE: both the above installation commands will install ABR in your currently referenced Python distribution. To play around with ABR without messing up your standard/default distribution, we recommed to create and use a dedicated environment, e.g., using Conda or virtualenv. (Optional) Setting up blender with a custom python installation \u00b6 Despite our best efforts, ABR depends, or might in the future depend, on external libraries that go beyond what blender is shipping. However, installing third party dependencies using pip might not directly work, depending on your blender version. It is, however, possible, to replace blender\u2019s python version with a locally installed variant. Here, we outline the steps that are required to setup a python that is installed in a local virtual environment and make it the one that blender will use on a 64bit computer. These steps might also be required if you intend to render datasets on a GPU Cluster that has specific needs for python version, dedicated PIP backends, etc. NOTE : Before you start changing blender as outlined below, we urge you to try ABR without changing blender\u2019s python! Installing blender \u00b6 The example below uses blender-2.80. However, this should also work for later blender versions. Important is that the python version that should replace blender\u2019s shipped version has the same major and minor version numbers. For instance, you should be able to replace a python 3.7.0 with python 3.7.5. We were only partially successful in replacing python when the minor version number is different (i.e. 3.7.0 vs 3.8.0) due to blender\u2019s internal bindings, which require certain variants of the package encodings . Download the 64bit linux version of blender to your local computer and unpack it in some suitable directory. The next few steps assume that you have the following layout of files in your home folder: $ ~/bin # folder with executables, scripts, etc. $ ~/bin/blender-2.80.d # un-packed blender download $ ~/bin/blender # symlink to ~/bin/blender-2.80.d/blender To create the symlink run $ ln -s blender-2.80.d/blender blender Make sure that ~/bin is on your path (you can add it e.g., through your ~/.bashrc ). To quickly test if the setup is correct you can try running blender from your command line which should start Blender\u2019s 2.80 GUI. As mentioned above, blender ships its own python binary. This leads to issues when trying to install third party libraries due to, e.g., numpy version mismatches. There are three ways of dealing with this: Replacing blender\u2019s python version with your own virtual environment, configuring the blender python version to be able to use pip or using conda. Replacing Blender\u2019s python version \u00b6 The following replaces the shipped python version with the python of a virtualenv. We assume that blender was installed as above to ~/bin/blender , and that you have virtualenv or virtualenvwrapper installed. $ mkvirtualenv blender-venv # This creates a new virtual environment. # The path to the venv depends on your system # setup. By default, it should end up either in # ~/.venvs, ~/.virtualenvs or something similar. # In the example here, we assume that virtualenvs # are created in ~/.venvs . # Note that this also activates the venv, # which should be indicated by # `(blender-env)` in front of PS1 (the dollar # sign that indicates your shell $). ( blender-venv ) $ cd bin/blender.d/2.80 ( blender-venv ) $ mv python original.python # make back up of shipped python ( blender-venv ) $ ln -s ~/venvs/blender-venv python ( blender-venv ) $ cd .. We can test if this worked by calling blender and dropping into a python console from the command line: ( blender-venv ) $ ./blender -b --python-console You can exit the shell with Ctrl-D. If the last step (running blender with an interactive python shell) failed, something went wrong. Most likely, you will have received an error which indicates that a certain package (encodings or initfsencoding) is missing our could not be loaded. Specifically, you might have received the following messages: Fatal Python error: initfsencoding: Unable to get the locale encoding ModuleNotFoundError: No module named 'encoding If this is the case, make sure that your virtualenv was created with a python3.7 virtualenv script, and neither with a python2 nor a python3.8 virtualenv. This could happen if you have a virtualenv script locally installed in ~/.local/bin, which points to a python2 environment. One viable workaround is to create a python3 environment from which you run the above commands, i.e. Create a python3 environment with your virtualenv installation, e.g. called \u2018py3bootstrap\u2019 Locally (i.e., inside the python3 environemnt) install virtualenv and virtualenvwrapper $ ( py3bootstrap ) pip install virtualenv virtualenvwrapper Now create your blender virtual environment $ ( py3bootstrap ) mkvirtualenv blender-venv Follow the steps above. If the aforementioned 4 steps do not work, try to create a python environment using an explicit call to the appropriate virtualenv: $ python3.7 .local/lib/python3.7/site-packages/virtualenv.py blender-env If this still does not solve the issue, please get in contact with us, and we try to help you out. Setting up Blender\u2019s python to work with pip \u00b6 Since version 2.80 blender\u2019s python distribution ships with ensurepip . This allows you to setup pip in blender. The instructions given here are loosely based on this StackOverflow post $ export BLENDER_PYTHON_DIR = path/to/blender/2.80/python/bin $ export BLENDER_PYTHON_PATH = $BLENDER_PYTHON_DIR /python3.7m $ ${ BLENDER_PYTHON_PATH } -m ensurepip $ ${ BLENDER_PYTHON_PATH } -m pip install -U pip $ # This is just convenience for better usability $ echo \"alias pip-blender=' ${ BLENDER_PYTHON_PATH } -m pip'\" >> ~/.bashrc $ echo \"export PATH=\\${PATH}: $BLENDER_PYTHON_DIR \" >> ~/.bashrc You can test this solution by running $ source ~/.bashrc && pip-blender --version $ # Should point to the blender python distribution Note This procedure has the advantage that you do not need to take care of creating a dedicated python environment and struggle with selecting the correct interpreter version. On the other hand, it directly modifies the original Blender\u2019s python distro. To minimize the risk of potential issues we suggest to make a copy of the original python distro. Testing your python installation \u00b6 The following instructions assume, that you did the virtualenv setup. If you have reconfigured blender\u2019s python version, you do not need to work in a virtual environment. Instead, replace all pip commands with the pip version of blenders\u2019 python distribution. If you followed this tutorial, this should be pip-blender If everything worked as it should, you can now install python packages within the newly created virtual environment with pip, which are then also available from within blender. For instance, to install numpy, imageio, and torch, simply run the following ( blender-venv ) $ pip install numpy imageio torch Running blender with an interactive shell, you should now be able to import numpy, torch, etc. ( blender-venv ) $ blender -b --python-console >>> import numpy, torch, imageio without getting an ImportError. If this worked out, you can finally install ABR in your local virtualenv by running from ABR root dir (where setup.py is located) ( blender-venv ) $ pip install . or, for the editable version ( blender-venv ) $ pip install -e . Using Conda \u00b6 Yet another option is to use conda as a virtual environement and package manager for python. We assume anaconda3 ( here or here ) is installed in your $HOME and available on you path. Make sure your version of anaconda python is >= 3.6 Create a conda environment by running $ conda create --name blender-venv python = 3 .7.5 imageio numpy Similar to explained when using virtualenv, symlink blender to the environment. That is, from within ~/bin/blender-2.80.d/2.80 run $ ln -s ~/anaconda3/env/blender-venv python To check whether this was successfull, run $ conda activate blender-venv ( blender-venv ) $ blender -b --python-console It this went through you should now be able to use ABR. Note The advantage of using conda rather than virtualenv is that any anaconda3 version allows you to select, as interpreter for your environemnt, python3.7.x.","title":"Installation"},{"location":"installation/#installation","text":"This page explains how to install ABR. In order for ABR to properly interact with blender, it might be necessary to replace the python distribution shipped with Blender as below explained. We suggest to first try w/o doing so and resort to it if nothing else works. Tested requirements The following instructions have been tested for a variety of environments: OS: Ubuntu 18.04 (Bionic Beaver) and 20.04 (Focal Fossa) Blender: >=2.80, <=2.91.2 python3.7(.x) Using a more recent version of Blender should be possible but we do not ensure it.","title":"Installation"},{"location":"installation/#installing-abr-as-a-python-package","text":"The easiest way to install (and later use) ABR can be done with pip . If you simply want to use ABR without adding your own scenes or without making modifications, the best way to install it so is to simply call pip install . from the root of ABR\u2019s source tree, i.e. the folder in which you can find the file setup.py . We call this the passive user mode . However, we expect users to be active . That is, we believe that, at some point, you might want to add your own scenes, make modifications to existing scenes, etc. If you are one of those active users , then you should rather install ABR in editable mode via pip install -e . from ABR\u2019s root folder. This way, you can still edit all files, add new scenes, without having to manually re-install ABR afterwards. There\u2019s also the possibility to use ABR without any installation procedure. More about this can be found in using . NOTE: both the above installation commands will install ABR in your currently referenced Python distribution. To play around with ABR without messing up your standard/default distribution, we recommed to create and use a dedicated environment, e.g., using Conda or virtualenv.","title":"Installing ABR as a python package"},{"location":"installation/#optional-setting-up-blender-with-a-custom-python-installation","text":"Despite our best efforts, ABR depends, or might in the future depend, on external libraries that go beyond what blender is shipping. However, installing third party dependencies using pip might not directly work, depending on your blender version. It is, however, possible, to replace blender\u2019s python version with a locally installed variant. Here, we outline the steps that are required to setup a python that is installed in a local virtual environment and make it the one that blender will use on a 64bit computer. These steps might also be required if you intend to render datasets on a GPU Cluster that has specific needs for python version, dedicated PIP backends, etc. NOTE : Before you start changing blender as outlined below, we urge you to try ABR without changing blender\u2019s python!","title":"(Optional) Setting up blender with a custom python installation"},{"location":"installation/#installing-blender","text":"The example below uses blender-2.80. However, this should also work for later blender versions. Important is that the python version that should replace blender\u2019s shipped version has the same major and minor version numbers. For instance, you should be able to replace a python 3.7.0 with python 3.7.5. We were only partially successful in replacing python when the minor version number is different (i.e. 3.7.0 vs 3.8.0) due to blender\u2019s internal bindings, which require certain variants of the package encodings . Download the 64bit linux version of blender to your local computer and unpack it in some suitable directory. The next few steps assume that you have the following layout of files in your home folder: $ ~/bin # folder with executables, scripts, etc. $ ~/bin/blender-2.80.d # un-packed blender download $ ~/bin/blender # symlink to ~/bin/blender-2.80.d/blender To create the symlink run $ ln -s blender-2.80.d/blender blender Make sure that ~/bin is on your path (you can add it e.g., through your ~/.bashrc ). To quickly test if the setup is correct you can try running blender from your command line which should start Blender\u2019s 2.80 GUI. As mentioned above, blender ships its own python binary. This leads to issues when trying to install third party libraries due to, e.g., numpy version mismatches. There are three ways of dealing with this: Replacing blender\u2019s python version with your own virtual environment, configuring the blender python version to be able to use pip or using conda.","title":"Installing blender"},{"location":"installation/#replacing-blenders-python-version","text":"The following replaces the shipped python version with the python of a virtualenv. We assume that blender was installed as above to ~/bin/blender , and that you have virtualenv or virtualenvwrapper installed. $ mkvirtualenv blender-venv # This creates a new virtual environment. # The path to the venv depends on your system # setup. By default, it should end up either in # ~/.venvs, ~/.virtualenvs or something similar. # In the example here, we assume that virtualenvs # are created in ~/.venvs . # Note that this also activates the venv, # which should be indicated by # `(blender-env)` in front of PS1 (the dollar # sign that indicates your shell $). ( blender-venv ) $ cd bin/blender.d/2.80 ( blender-venv ) $ mv python original.python # make back up of shipped python ( blender-venv ) $ ln -s ~/venvs/blender-venv python ( blender-venv ) $ cd .. We can test if this worked by calling blender and dropping into a python console from the command line: ( blender-venv ) $ ./blender -b --python-console You can exit the shell with Ctrl-D. If the last step (running blender with an interactive python shell) failed, something went wrong. Most likely, you will have received an error which indicates that a certain package (encodings or initfsencoding) is missing our could not be loaded. Specifically, you might have received the following messages: Fatal Python error: initfsencoding: Unable to get the locale encoding ModuleNotFoundError: No module named 'encoding If this is the case, make sure that your virtualenv was created with a python3.7 virtualenv script, and neither with a python2 nor a python3.8 virtualenv. This could happen if you have a virtualenv script locally installed in ~/.local/bin, which points to a python2 environment. One viable workaround is to create a python3 environment from which you run the above commands, i.e. Create a python3 environment with your virtualenv installation, e.g. called \u2018py3bootstrap\u2019 Locally (i.e., inside the python3 environemnt) install virtualenv and virtualenvwrapper $ ( py3bootstrap ) pip install virtualenv virtualenvwrapper Now create your blender virtual environment $ ( py3bootstrap ) mkvirtualenv blender-venv Follow the steps above. If the aforementioned 4 steps do not work, try to create a python environment using an explicit call to the appropriate virtualenv: $ python3.7 .local/lib/python3.7/site-packages/virtualenv.py blender-env If this still does not solve the issue, please get in contact with us, and we try to help you out.","title":"Replacing Blender's python version"},{"location":"installation/#setting-up-blenders-python-to-work-with-pip","text":"Since version 2.80 blender\u2019s python distribution ships with ensurepip . This allows you to setup pip in blender. The instructions given here are loosely based on this StackOverflow post $ export BLENDER_PYTHON_DIR = path/to/blender/2.80/python/bin $ export BLENDER_PYTHON_PATH = $BLENDER_PYTHON_DIR /python3.7m $ ${ BLENDER_PYTHON_PATH } -m ensurepip $ ${ BLENDER_PYTHON_PATH } -m pip install -U pip $ # This is just convenience for better usability $ echo \"alias pip-blender=' ${ BLENDER_PYTHON_PATH } -m pip'\" >> ~/.bashrc $ echo \"export PATH=\\${PATH}: $BLENDER_PYTHON_DIR \" >> ~/.bashrc You can test this solution by running $ source ~/.bashrc && pip-blender --version $ # Should point to the blender python distribution Note This procedure has the advantage that you do not need to take care of creating a dedicated python environment and struggle with selecting the correct interpreter version. On the other hand, it directly modifies the original Blender\u2019s python distro. To minimize the risk of potential issues we suggest to make a copy of the original python distro.","title":"Setting up Blender's python to work with pip"},{"location":"installation/#testing-your-python-installation","text":"The following instructions assume, that you did the virtualenv setup. If you have reconfigured blender\u2019s python version, you do not need to work in a virtual environment. Instead, replace all pip commands with the pip version of blenders\u2019 python distribution. If you followed this tutorial, this should be pip-blender If everything worked as it should, you can now install python packages within the newly created virtual environment with pip, which are then also available from within blender. For instance, to install numpy, imageio, and torch, simply run the following ( blender-venv ) $ pip install numpy imageio torch Running blender with an interactive shell, you should now be able to import numpy, torch, etc. ( blender-venv ) $ blender -b --python-console >>> import numpy, torch, imageio without getting an ImportError. If this worked out, you can finally install ABR in your local virtualenv by running from ABR root dir (where setup.py is located) ( blender-venv ) $ pip install . or, for the editable version ( blender-venv ) $ pip install -e .","title":"Testing your python installation"},{"location":"installation/#using-conda","text":"Yet another option is to use conda as a virtual environement and package manager for python. We assume anaconda3 ( here or here ) is installed in your $HOME and available on you path. Make sure your version of anaconda python is >= 3.6 Create a conda environment by running $ conda create --name blender-venv python = 3 .7.5 imageio numpy Similar to explained when using virtualenv, symlink blender to the environment. That is, from within ~/bin/blender-2.80.d/2.80 run $ ln -s ~/anaconda3/env/blender-venv python To check whether this was successfull, run $ conda activate blender-venv ( blender-venv ) $ blender -b --python-console It this went through you should now be able to use ABR. Note The advantage of using conda rather than virtualenv is that any anaconda3 version allows you to select, as interpreter for your environemnt, python3.7.x.","title":"Using Conda"},{"location":"license/","text":"License \u00b6 AMIRA Blender Rendering is released under the Apache-2.0 license. Please refer to the LICENSE notice file located in the root directory of the project repo.","title":"License"},{"location":"license/#license","text":"AMIRA Blender Rendering is released under the Apache-2.0 license. Please refer to the LICENSE notice file located in the root directory of the project repo.","title":"License"},{"location":"tests/","text":"Running Tests \u00b6 ABR relies on the standard unittest python framework. Multiple tests are already implemented in order to ensure a minimal code coverage. However, since some tests need to load Blender\u2019s python API (bpy), it is not possible to use the standard discover option for unittests. To overcome this we provide a dedicated script. For ABR root directory, you just need to run ./scripts/run_tests --abr-path ~/amira_blender_rendering Implementing your own tests \u00b6 In case you directly contribute to ABR, it is of course possible (and recommended) to implement additional tests. To do that you need to: Locate (or create a new one) the most suitable directory in $ABR/tests where to put your test files Implement your tests according the standard unittest framework. You can take inspiration from existing tests Similar to existing tests, add your own to tests.__main__::import_and_run_implemented_tests Afterwards you can run tests as above expalined.","title":"Tests"},{"location":"tests/#running-tests","text":"ABR relies on the standard unittest python framework. Multiple tests are already implemented in order to ensure a minimal code coverage. However, since some tests need to load Blender\u2019s python API (bpy), it is not possible to use the standard discover option for unittests. To overcome this we provide a dedicated script. For ABR root directory, you just need to run ./scripts/run_tests --abr-path ~/amira_blender_rendering","title":"Running Tests"},{"location":"tests/#implementing-your-own-tests","text":"In case you directly contribute to ABR, it is of course possible (and recommended) to implement additional tests. To do that you need to: Locate (or create a new one) the most suitable directory in $ABR/tests where to put your test files Implement your tests according the standard unittest framework. You can take inspiration from existing tests Similar to existing tests, add your own to tests.__main__::import_and_run_implemented_tests Afterwards you can run tests as above expalined.","title":"Implementing your own tests"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 I get a \u201cCUDA out of memory\u201d exception when rendering, what shall I do? Check if the models and scenarios you try to render can be rendered on your computer and do not exceed your GPU\u2019s RAM limit. To test this, open the file in blender and try to render it from the GUI. Blender should indicate how much memory it requires for rendering. Compare this value with your GPU RAM. I get a \u201cCUDA out of memory\u201d exception, but I have a GPU with gazillion GB of RAM! There appears to be an issue for certain scenarios and models that have many light sources and different procedural textures. In this case, it happens every once in a while, that the BVH that is generated by blender does not fit into the GPU. It is currently unclear if this is a blender problem, a driver issue, or something unrelated. If you experience this issue, try to re-run rendering (both within blender, or the scripts), as this issue appears to happen only sporadically. If rendering starts from the script, it will finish - rendering and moving data to the GPU is performed only in the beginning. If re-running does not solve this issue, try to reduce the amount of light sources and models in your scenario. Why does forward simulating frames require lots of time? The reason for this is that, to drop objects physically correctly, we use blenders physics engine (in turn, this is bullet physics), and set the shape transform in blender to \u201cMesh\u201d. The time the physics simulation takes depends crucially on the model you use, an in particular the number of vertices your model has. The rule of thumb is: fewer vertices are faster to simulate forward. Conclusively, either wait until it\u2019s done, or use a low-poly model of your target object. I set up my custom sceneario and its python backend but when I try to run abrgen I get a \u201cRuntime\u201d error telling that ABR does not know my scenario! Most likely you forgot to let ABR automatically register your scenario by using abr_scenes.register(name=, type=) . Please refer to our tutorial If the above is not the case, in your configuration .cfg file make sure that dataset.scene_type is set to be exactly equal to your selected _scene_name (see our tutorial ).","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"I get a \u201cCUDA out of memory\u201d exception when rendering, what shall I do? Check if the models and scenarios you try to render can be rendered on your computer and do not exceed your GPU\u2019s RAM limit. To test this, open the file in blender and try to render it from the GUI. Blender should indicate how much memory it requires for rendering. Compare this value with your GPU RAM. I get a \u201cCUDA out of memory\u201d exception, but I have a GPU with gazillion GB of RAM! There appears to be an issue for certain scenarios and models that have many light sources and different procedural textures. In this case, it happens every once in a while, that the BVH that is generated by blender does not fit into the GPU. It is currently unclear if this is a blender problem, a driver issue, or something unrelated. If you experience this issue, try to re-run rendering (both within blender, or the scripts), as this issue appears to happen only sporadically. If rendering starts from the script, it will finish - rendering and moving data to the GPU is performed only in the beginning. If re-running does not solve this issue, try to reduce the amount of light sources and models in your scenario. Why does forward simulating frames require lots of time? The reason for this is that, to drop objects physically correctly, we use blenders physics engine (in turn, this is bullet physics), and set the shape transform in blender to \u201cMesh\u201d. The time the physics simulation takes depends crucially on the model you use, an in particular the number of vertices your model has. The rule of thumb is: fewer vertices are faster to simulate forward. Conclusively, either wait until it\u2019s done, or use a low-poly model of your target object. I set up my custom sceneario and its python backend but when I try to run abrgen I get a \u201cRuntime\u201d error telling that ABR does not know my scenario! Most likely you forgot to let ABR automatically register your scenario by using abr_scenes.register(name=, type=) . Please refer to our tutorial If the above is not the case, in your configuration .cfg file make sure that dataset.scene_type is set to be exactly equal to your selected _scene_name (see our tutorial ).","title":"Troubleshooting"},{"location":"using/","text":"Using ABR \u00b6 ABR was developed to be used in headless fashion to render datasets from pre-defined blender files. For this, we provide the single (command line) entry point abrgen , to which a user has to pass a configuration file (see Configuration and possibly other command line arguments, and which will then invoke blender in the correct manner. Using ABR with installation \u00b6 If you followed our installation and installed ABR, e.g., via pip install or pip install -e , you can use abrgen directly from your command line. For instance, the following will start producing a dataset specified according to the configuration my_config.cfg : $ abrgen --config my_config.cfg If you would like to temporarily alter any configuration option without changing the configuration file, you can simply pass this as a command line argument to abrgen . For example, assume you would like to change the number of images that are produced, which is usually stored in the configuration file in section dataset and has the name image_count (see our base configuration for more generally available options). To change this number temporarily, you can invoke abrgen with $ abrgen --config my_config.cfg --dataset.image_count 123 In case you want to discover all options that are available for a certain scene or Configuration file, you can pass --help along to abrgen : $ abrgen --config my_config.cfg --help Note that this requires a configuration file which specifies at least the scene type, and will list all parameters that you can modify for the specified scene. More information about specifying scene types can be found in our base configuration . Instead, in case you want to discover all available arguments to call abrgen with you can run $ abrgen --help Using ABR without installation \u00b6 Sometimes you might not want to or cannot install ABR, or you cannot even run abrgen due to permission issues on the target system. As long as you can invoke blender , you can still make use of ABR, though. In the first case, that is when you can still run abrgen , you can tell it where to find the ABR package using the --abr-path command line argument: $ abrgen --abr-path /path/to/ABR/src --config my_config.cfg In the second case, or if you would like to circumvent using abrgen , you can also directly invoke blender. For this to work, you need to locate the file render_dataset.py inside the ABR source tree (it should be located in $ABR/src/amira_blender_rendering/cli where $ABR should contain the path to ABR root directory), and call blender with the following options: $ blender -b -P /path/to/render_dataset.py -- --abr-path /path/to/ABR/src --config my_config.cfg Using ABR for headless rendering on a GPU cluster \u00b6 ABR was developed with headless rendering on a GPU cluster in mind. Hence, there is no significant difference between setting up ABR locally on your computer, or on a remote system. For more details about how to install ABR, please have a look at installation , and for more information about how to use it, see the sections above. Nevertheless, we here outline the steps that are often required or recommended to get rendering going on a headless GPU server. The examples below assume that your GPU server has a working anaconda installation. We also assume that you follow good practices and isolate your work into separate virtual environments. create a new conda environment for python 3.7 $ conda create --name py37 python = 3 .7 This creates a new virtual environment with name py37 . In our case, anaconda create this virtual env in /software/USERNAME/anaconda/envs/py37 . Please note the path that conda reported, as it will be relevant later on. If you haven\u2019t done so already, fetch blender in a version that is supported by ABR, i.e. >=2.80, and copy it to your GPU cluster. Make sure that the blender binary is on your PATH. Replace blender\u2019s python with the conda environment\u2019s python as described in installation , and run blender to test if it works: $ blender -b --python-console This should give you an interactive python shell. Note that you can ignore any ALSA errors that might get printed, as we don\u2019t consider sound in our datasets, and GPU clusters often don\u2019t ship with sound cards. Activate your new conda environment and install ABR\u2019s dependencies via conda or pip. The example below uses pip. $ conda activate py37 ( py37 ) $ cd /path/to/amira_blender_rendering ( py37 ) $ pip install -r requirements.txt If you haven\u2019t done so already, or if your GPU cluster does not provide a certain location for common datasets, you might wish to copy required datasets (e.g. OpenImages) to a folder that you know and which you can specify in Configuration files. A good and common practice is to use global variables, e.g. $DATASET_DIR , that you set in your .bashrc or .zshrc (or whichever shell you use) and which point to folders with such data. This way, you can simply copy your local Configuration files to your GPU cluster without having to change relevant paths. Note that you can make use of all global variables in Configuration files, e.g. when specifying environment textures, because we expand all such variables before trying to access a path. Finally, use amira_blender_rendering to generate your dataset, e.g. $ abrgen --config config/my_config.cfg Notice that you do not need to have your environment active to do so. This is because abrgen and, in turn, blender, will already point to it. Environment variables \u00b6 As mentioned in point 6. above, note that some scenes and/or configurations might require you to setup global variables. Here\u2019s a non-exhaustive list of the variables that we usually use (Name | Description): $OUTDIR : path to output directory where results are stored $AMIRA_DATASETS or DATA : Path to datasets, such as the one produced here, or OpenImagesV4 $AMIRA_DATA_GFX or $DATA_GFX : Path to graphics data $AMIRA_BLENDER_RENDERING_ASSETS : Path to additional assets, such as textures Rendering modes \u00b6 Currently, for some of the ready available scenes, ABR offers two different rendering modes (DEFAULT, MULTIVIEW) which can be selected at deployment time by running abrgen with the flag --render-mode followed by the name of the mode. DEFAULT refers to the default rendering mode. That is, if no flag is explicitly selected, this mode is automatically called. In this mode we usually render one camera view (static-camera) per each (random) scene. Note that the exact behavior of the render mode depends on the so-called scene backend . MULTIVIEW usually refers to the case when we render multiple camera views for the same (random) scene. That is the camera is moved around in 3D space and images are rendered from each of these camera locations. Note that how camera locations are selected depends on specific configuration values to be set in the .cfg file abrgen is called with. For specific behaviors, refer to the configurations docs.","title":"Using"},{"location":"using/#using-abr","text":"ABR was developed to be used in headless fashion to render datasets from pre-defined blender files. For this, we provide the single (command line) entry point abrgen , to which a user has to pass a configuration file (see Configuration and possibly other command line arguments, and which will then invoke blender in the correct manner.","title":"Using ABR"},{"location":"using/#using-abr-with-installation","text":"If you followed our installation and installed ABR, e.g., via pip install or pip install -e , you can use abrgen directly from your command line. For instance, the following will start producing a dataset specified according to the configuration my_config.cfg : $ abrgen --config my_config.cfg If you would like to temporarily alter any configuration option without changing the configuration file, you can simply pass this as a command line argument to abrgen . For example, assume you would like to change the number of images that are produced, which is usually stored in the configuration file in section dataset and has the name image_count (see our base configuration for more generally available options). To change this number temporarily, you can invoke abrgen with $ abrgen --config my_config.cfg --dataset.image_count 123 In case you want to discover all options that are available for a certain scene or Configuration file, you can pass --help along to abrgen : $ abrgen --config my_config.cfg --help Note that this requires a configuration file which specifies at least the scene type, and will list all parameters that you can modify for the specified scene. More information about specifying scene types can be found in our base configuration . Instead, in case you want to discover all available arguments to call abrgen with you can run $ abrgen --help","title":"Using ABR with installation"},{"location":"using/#using-abr-without-installation","text":"Sometimes you might not want to or cannot install ABR, or you cannot even run abrgen due to permission issues on the target system. As long as you can invoke blender , you can still make use of ABR, though. In the first case, that is when you can still run abrgen , you can tell it where to find the ABR package using the --abr-path command line argument: $ abrgen --abr-path /path/to/ABR/src --config my_config.cfg In the second case, or if you would like to circumvent using abrgen , you can also directly invoke blender. For this to work, you need to locate the file render_dataset.py inside the ABR source tree (it should be located in $ABR/src/amira_blender_rendering/cli where $ABR should contain the path to ABR root directory), and call blender with the following options: $ blender -b -P /path/to/render_dataset.py -- --abr-path /path/to/ABR/src --config my_config.cfg","title":"Using ABR without installation"},{"location":"using/#using-abr-for-headless-rendering-on-a-gpu-cluster","text":"ABR was developed with headless rendering on a GPU cluster in mind. Hence, there is no significant difference between setting up ABR locally on your computer, or on a remote system. For more details about how to install ABR, please have a look at installation , and for more information about how to use it, see the sections above. Nevertheless, we here outline the steps that are often required or recommended to get rendering going on a headless GPU server. The examples below assume that your GPU server has a working anaconda installation. We also assume that you follow good practices and isolate your work into separate virtual environments. create a new conda environment for python 3.7 $ conda create --name py37 python = 3 .7 This creates a new virtual environment with name py37 . In our case, anaconda create this virtual env in /software/USERNAME/anaconda/envs/py37 . Please note the path that conda reported, as it will be relevant later on. If you haven\u2019t done so already, fetch blender in a version that is supported by ABR, i.e. >=2.80, and copy it to your GPU cluster. Make sure that the blender binary is on your PATH. Replace blender\u2019s python with the conda environment\u2019s python as described in installation , and run blender to test if it works: $ blender -b --python-console This should give you an interactive python shell. Note that you can ignore any ALSA errors that might get printed, as we don\u2019t consider sound in our datasets, and GPU clusters often don\u2019t ship with sound cards. Activate your new conda environment and install ABR\u2019s dependencies via conda or pip. The example below uses pip. $ conda activate py37 ( py37 ) $ cd /path/to/amira_blender_rendering ( py37 ) $ pip install -r requirements.txt If you haven\u2019t done so already, or if your GPU cluster does not provide a certain location for common datasets, you might wish to copy required datasets (e.g. OpenImages) to a folder that you know and which you can specify in Configuration files. A good and common practice is to use global variables, e.g. $DATASET_DIR , that you set in your .bashrc or .zshrc (or whichever shell you use) and which point to folders with such data. This way, you can simply copy your local Configuration files to your GPU cluster without having to change relevant paths. Note that you can make use of all global variables in Configuration files, e.g. when specifying environment textures, because we expand all such variables before trying to access a path. Finally, use amira_blender_rendering to generate your dataset, e.g. $ abrgen --config config/my_config.cfg Notice that you do not need to have your environment active to do so. This is because abrgen and, in turn, blender, will already point to it.","title":"Using ABR for headless rendering on a GPU cluster"},{"location":"using/#environment-variables","text":"As mentioned in point 6. above, note that some scenes and/or configurations might require you to setup global variables. Here\u2019s a non-exhaustive list of the variables that we usually use (Name | Description): $OUTDIR : path to output directory where results are stored $AMIRA_DATASETS or DATA : Path to datasets, such as the one produced here, or OpenImagesV4 $AMIRA_DATA_GFX or $DATA_GFX : Path to graphics data $AMIRA_BLENDER_RENDERING_ASSETS : Path to additional assets, such as textures","title":"Environment variables"},{"location":"using/#rendering-modes","text":"Currently, for some of the ready available scenes, ABR offers two different rendering modes (DEFAULT, MULTIVIEW) which can be selected at deployment time by running abrgen with the flag --render-mode followed by the name of the mode. DEFAULT refers to the default rendering mode. That is, if no flag is explicitly selected, this mode is automatically called. In this mode we usually render one camera view (static-camera) per each (random) scene. Note that the exact behavior of the render mode depends on the so-called scene backend . MULTIVIEW usually refers to the case when we render multiple camera views for the same (random) scene. That is the camera is moved around in 3D space and images are rendered from each of these camera locations. Note that how camera locations are selected depends on specific configuration values to be set in the .cfg file abrgen is called with. For specific behaviors, refer to the configurations docs.","title":"Rendering modes"},{"location":"configs/baseconfiguration/","text":"Base Configuration \u00b6 The base configuration is supported by all scenarios. It consists of several namespaces, which are described in detail below. dataset \u00b6 The dataset namespace contains information about a dataset such as number of images, as well as the output directory where data will be written to. [ dataset ] # Specify how many images should be rendered image_count = 5 # Depending on the rendering mode it is also possible to set scene # and camera view counts. Note that setting these values might affect # the total number of rendered image and, in turn, image_count. # As an example, if supported, setting scene_cout = 5, view_count = 5 in # \"multiview\" render mode will result in image_count = 25 scene_count = view_count = # Specify the base path where data will be written to. Note that this is a base # path, to which additional information will be added such as Scenario-Number # and Camera-Name base_path = $ OUTDIR / WorkstationScenarios - Train # specify the scene type scene_type = WorkstationScenarios camera_info \u00b6 Camera information is placed in the camera_info namespace. It contains settings for image width and height, as well as (optional) intrinsic camera information. [ camera_info ] # In this section you specify the camera information, which will have a direct # impact on rendering results. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy # Where the fx, fy values represented focal lengths, and cx, cy defines the # camera's principal point. # # You can extract fx, fy, cx, cy from a calibration matrix K: # # fx s cx # K = 0 fy cy # 0 0 1 # # Note, however, that the values in your calibration matrix or intrinsics # specification might not end up in proper render resolutions. For instance, # this is the case in the example below, which would result in a rendering # resolution of about 1320.98 x 728.08. Blender will round these values to # suitable integer values. As a consequence, even if you set width and height # above to 0, the effective intrinsics that blender uses might be slightly # different from your K. # # To accomodate this 'issue', amira_blender_rendering will write a value # 'effective_intrinsics' to the configuration as soon as setting up cameras and # rendering is done. Recall that all configurations will be stored alongside the # created dataset, so you can easily retrieve the effective_intrinsics in # downstream applications intrinsics = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # A default camera in blender with 0 rotation applied to its transform looks # along the -Z direction. Blender's modelling viewport, however, assumes that # the surface plane is spanned by X and Y, where X indicates left/right. This # can be observed by putting the modelling viewport into the front viewpoint # (Numpad 1). Then, the viewport looks along the Y direction. # # As a consequence, the relative rotation between a camera image and an object # is only 0 when the camera would look onto the top of the object. Note that # this is rather unintuitive, as most people would expect that the relative # rotation is 0 when the camera looks at the front of an object. # # To accomodate for this, users can set their preferred 'zeroing' rotation # by using the following configuration parameter encoding rotations # around x, y and z-axis, respectively, in degrees. # # As an example, a value of 90, 0, 0 will apply a rotation of 90[deg] around x # when computing the relative rotation between the camera and an object in the # in the camera reference frame. zeroing = 0.0 , 0.0 , 0.0 # We allow to set camera parameters also using additional values. These are: # The sensor width in mm (if not available, set to 0.0) sensor_width = # The camera focal lenght in mm (if not available, set to 0.0) focal_length = # The camera Horizontal Field-of-View in degrees (if not available, set to 0.0) hfov = # Additionally, it is possible to determin how to compute the camera setup if only # instrinsics values are give among \"fov\" and \"mm\" (default is \"mm\"). intrinsics_conversion_mode = render_setup \u00b6 The render_setup namespace is used to configure how blender\u2019s render backend behaves, or which render backend to use. [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 64 # allow occlusions of target objects (true, false) allow_occlusions = False # select bit size of RGB images between 8 bit and 16 bit (default) color_depth = 16 # toggle motion blur (True, False (defualt)) during rendering. # Notice that, this might not heavily affect # your render output if the rendered scene is standing still. motion_blue = False debugging \u00b6 The debug namespace can be used to toggle debug functionatilies. For scene specific flags refer to the desider scene. [ debug ] # activate debug logs and print-outs (true, false) enabled = False postprocess \u00b6 The postprocess namespace can be used to implement functionatilies during postprocess and/or after the rendering phase [ postprocess ] # By default Blender uses a perfect pinhole camera models and its output depth maps # contain indeed ranges (in meters saved as .exr files). For this reasons, (rectified) depth # maps (saved as png files) are computed during postprocessing. During generation we allow to # select the output scale to convert range to depth. Default is 1e4 = .1mm depth_scale = # During post processing it might happen that object visibility information (which are computed # using ray-casting) and the corresponding object mask do not correspond (ie. the mask is empty). # This might happen due to image resolution: the visible portion of the object is not big enough # for a single pixel. Since, for how seldom, this behavior can happen, we allow, to overwrite # visibility information based on the computed mask (defualt is False). visibility_from_mask = # If requested, the disparity between a set of parallel cameras can be computed. Default is False compute_disparity = # Disparity is computed only on given cameras (chosen among those set in scene_setup.cameras) parallel_cameras = [] # Disparity maps require a baseline value (in mm) between the selected cameras. Default is 0 parallel_cameras_baseline_mm =","title":"Base Config"},{"location":"configs/baseconfiguration/#base-configuration","text":"The base configuration is supported by all scenarios. It consists of several namespaces, which are described in detail below.","title":"Base Configuration"},{"location":"configs/baseconfiguration/#dataset","text":"The dataset namespace contains information about a dataset such as number of images, as well as the output directory where data will be written to. [ dataset ] # Specify how many images should be rendered image_count = 5 # Depending on the rendering mode it is also possible to set scene # and camera view counts. Note that setting these values might affect # the total number of rendered image and, in turn, image_count. # As an example, if supported, setting scene_cout = 5, view_count = 5 in # \"multiview\" render mode will result in image_count = 25 scene_count = view_count = # Specify the base path where data will be written to. Note that this is a base # path, to which additional information will be added such as Scenario-Number # and Camera-Name base_path = $ OUTDIR / WorkstationScenarios - Train # specify the scene type scene_type = WorkstationScenarios","title":"dataset"},{"location":"configs/baseconfiguration/#camera_info","text":"Camera information is placed in the camera_info namespace. It contains settings for image width and height, as well as (optional) intrinsic camera information. [ camera_info ] # In this section you specify the camera information, which will have a direct # impact on rendering results. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy # Where the fx, fy values represented focal lengths, and cx, cy defines the # camera's principal point. # # You can extract fx, fy, cx, cy from a calibration matrix K: # # fx s cx # K = 0 fy cy # 0 0 1 # # Note, however, that the values in your calibration matrix or intrinsics # specification might not end up in proper render resolutions. For instance, # this is the case in the example below, which would result in a rendering # resolution of about 1320.98 x 728.08. Blender will round these values to # suitable integer values. As a consequence, even if you set width and height # above to 0, the effective intrinsics that blender uses might be slightly # different from your K. # # To accomodate this 'issue', amira_blender_rendering will write a value # 'effective_intrinsics' to the configuration as soon as setting up cameras and # rendering is done. Recall that all configurations will be stored alongside the # created dataset, so you can easily retrieve the effective_intrinsics in # downstream applications intrinsics = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # A default camera in blender with 0 rotation applied to its transform looks # along the -Z direction. Blender's modelling viewport, however, assumes that # the surface plane is spanned by X and Y, where X indicates left/right. This # can be observed by putting the modelling viewport into the front viewpoint # (Numpad 1). Then, the viewport looks along the Y direction. # # As a consequence, the relative rotation between a camera image and an object # is only 0 when the camera would look onto the top of the object. Note that # this is rather unintuitive, as most people would expect that the relative # rotation is 0 when the camera looks at the front of an object. # # To accomodate for this, users can set their preferred 'zeroing' rotation # by using the following configuration parameter encoding rotations # around x, y and z-axis, respectively, in degrees. # # As an example, a value of 90, 0, 0 will apply a rotation of 90[deg] around x # when computing the relative rotation between the camera and an object in the # in the camera reference frame. zeroing = 0.0 , 0.0 , 0.0 # We allow to set camera parameters also using additional values. These are: # The sensor width in mm (if not available, set to 0.0) sensor_width = # The camera focal lenght in mm (if not available, set to 0.0) focal_length = # The camera Horizontal Field-of-View in degrees (if not available, set to 0.0) hfov = # Additionally, it is possible to determin how to compute the camera setup if only # instrinsics values are give among \"fov\" and \"mm\" (default is \"mm\"). intrinsics_conversion_mode =","title":"camera_info"},{"location":"configs/baseconfiguration/#render_setup","text":"The render_setup namespace is used to configure how blender\u2019s render backend behaves, or which render backend to use. [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 64 # allow occlusions of target objects (true, false) allow_occlusions = False # select bit size of RGB images between 8 bit and 16 bit (default) color_depth = 16 # toggle motion blur (True, False (defualt)) during rendering. # Notice that, this might not heavily affect # your render output if the rendered scene is standing still. motion_blue = False","title":"render_setup"},{"location":"configs/baseconfiguration/#debugging","text":"The debug namespace can be used to toggle debug functionatilies. For scene specific flags refer to the desider scene. [ debug ] # activate debug logs and print-outs (true, false) enabled = False","title":"debugging"},{"location":"configs/baseconfiguration/#postprocess","text":"The postprocess namespace can be used to implement functionatilies during postprocess and/or after the rendering phase [ postprocess ] # By default Blender uses a perfect pinhole camera models and its output depth maps # contain indeed ranges (in meters saved as .exr files). For this reasons, (rectified) depth # maps (saved as png files) are computed during postprocessing. During generation we allow to # select the output scale to convert range to depth. Default is 1e4 = .1mm depth_scale = # During post processing it might happen that object visibility information (which are computed # using ray-casting) and the corresponding object mask do not correspond (ie. the mask is empty). # This might happen due to image resolution: the visible portion of the object is not big enough # for a single pixel. Since, for how seldom, this behavior can happen, we allow, to overwrite # visibility information based on the computed mask (defualt is False). visibility_from_mask = # If requested, the disparity between a set of parallel cameras can be computed. Default is False compute_disparity = # Disparity is computed only on given cameras (chosen among those set in scene_setup.cameras) parallel_cameras = [] # Disparity maps require a baseline value (in mm) between the selected cameras. Default is 0 parallel_cameras_baseline_mm =","title":"postprocess"},{"location":"configs/multiview/","text":"Multiview Configurations \u00b6 Multiview configurations can be used to select and setup certain camera motions during multiview render mode . A prescribed mode and its corresponding configuration can be selected in the config file setting multiview_setup.mode and multiview_setup.mode_cfg , respectively. Notes The number of views along the selected motion is controlled by dataset.view_count By default, the camera views are offset around the inital camera location. This can be disabled by setting multiview_setup.offset=False . Multiview configurations control only location of cameras and not their rotation. In the available scenes, all cameras are rigged so to always look at a certain 3D location. If you plan to develop your custom scene from scratch, it might be worth to consider doing something similar. The following example assumes random as a mode. [ multiview_setup ] mode = random mode_cfg = Modes \u00b6 Currently we provide the following modes: random : select camera locations randomly in 3D space bezier : select camera locations along a bezier curve circle : select camera locations along a circluar trajectory embedded in x-y world plane wave : select camera locations along a sinusoidal in (z) and circular (in x-y) trajectory viewsphere : select camera locations from an upper viewsphere piecewiselinear : select camera locations from a piece-wise linear trajectory Mode Configurations \u00b6 Each mode comes with its specific configurations. Here is an overview. [ multiview_setup ] # Points are generated according to a 3D multivariate Gaussian distribution mode = random mode_cfg . base_location = # offset value (mean). Default: 0 mode_cfg . scale = # standard deviation. Default: 1 [ multiview_setup ] mode = bezier # Three 3D control points to define the bezier curve. # Rrandomly selected, if none given mode_cfg . p0 = # Default: 0 mode_cfg . p1 = # Default: random mode_cfg . p2 = # Defautl: random # Where to start and end along the curve assuming it of lenght 1. mode_cfg . start = # Default: 0 mode_cfg . stop = # Default: 1 [ multiview_setup ] # The trajectory is embedded in x-y world plane mode = circle mode_cfg . radius = # self explanatory. Default: 1 mode_cfg . center = # self explanatory. Default: 0 [ multiview_setup ] # The \"wave\" mode forsees a trajectory which is circular in an x-y world plane # and sinusoidal along z world-axis. # This defines a trajectory that while circling around moves the camera up and down. mode = wave # as for the circle, radius anc center values mode_cfg . radius = # Default: 1 mode_cfg . center = # Default: 0 # in addition to the circle mode_cfg . frequency = # frequency of the sinusodial curve. Deafault: 1 mode_cfg . amplitude = # amplitude along -z of the sinusoidal curve. Default: 1 [ multiview_setup ] mode = viewsphere mode_cfg . scale = # approx. max radius of the viewsphere. Default 1 mode_cfg . bias = # center of the view sphere. Default: [0, 0, 1.5] [ multiview_setup ] mode = piecewiselinear # List of control points that defines piece-wise linear chunks of the trajectory mode_cfg . points = # Default [[0, 0, 0], [1, 1, 1]]","title":"Multi view rendering"},{"location":"configs/multiview/#multiview-configurations","text":"Multiview configurations can be used to select and setup certain camera motions during multiview render mode . A prescribed mode and its corresponding configuration can be selected in the config file setting multiview_setup.mode and multiview_setup.mode_cfg , respectively. Notes The number of views along the selected motion is controlled by dataset.view_count By default, the camera views are offset around the inital camera location. This can be disabled by setting multiview_setup.offset=False . Multiview configurations control only location of cameras and not their rotation. In the available scenes, all cameras are rigged so to always look at a certain 3D location. If you plan to develop your custom scene from scratch, it might be worth to consider doing something similar. The following example assumes random as a mode. [ multiview_setup ] mode = random mode_cfg =","title":"Multiview Configurations"},{"location":"configs/multiview/#modes","text":"Currently we provide the following modes: random : select camera locations randomly in 3D space bezier : select camera locations along a bezier curve circle : select camera locations along a circluar trajectory embedded in x-y world plane wave : select camera locations along a sinusoidal in (z) and circular (in x-y) trajectory viewsphere : select camera locations from an upper viewsphere piecewiselinear : select camera locations from a piece-wise linear trajectory","title":"Modes"},{"location":"configs/multiview/#mode-configurations","text":"Each mode comes with its specific configurations. Here is an overview. [ multiview_setup ] # Points are generated according to a 3D multivariate Gaussian distribution mode = random mode_cfg . base_location = # offset value (mean). Default: 0 mode_cfg . scale = # standard deviation. Default: 1 [ multiview_setup ] mode = bezier # Three 3D control points to define the bezier curve. # Rrandomly selected, if none given mode_cfg . p0 = # Default: 0 mode_cfg . p1 = # Default: random mode_cfg . p2 = # Defautl: random # Where to start and end along the curve assuming it of lenght 1. mode_cfg . start = # Default: 0 mode_cfg . stop = # Default: 1 [ multiview_setup ] # The trajectory is embedded in x-y world plane mode = circle mode_cfg . radius = # self explanatory. Default: 1 mode_cfg . center = # self explanatory. Default: 0 [ multiview_setup ] # The \"wave\" mode forsees a trajectory which is circular in an x-y world plane # and sinusoidal along z world-axis. # This defines a trajectory that while circling around moves the camera up and down. mode = wave # as for the circle, radius anc center values mode_cfg . radius = # Default: 1 mode_cfg . center = # Default: 0 # in addition to the circle mode_cfg . frequency = # frequency of the sinusodial curve. Deafault: 1 mode_cfg . amplitude = # amplitude along -z of the sinusoidal curve. Default: 1 [ multiview_setup ] mode = viewsphere mode_cfg . scale = # approx. max radius of the viewsphere. Default 1 mode_cfg . bias = # center of the view sphere. Default: [0, 0, 1.5] [ multiview_setup ] mode = piecewiselinear # List of control points that defines piece-wise linear chunks of the trajectory mode_cfg . points = # Default [[0, 0, 0], [1, 1, 1]]","title":"Mode Configurations"},{"location":"configs/overview/","text":"Overview \u00b6 Nested INI Files \u00b6 The ABR pipeline is configured using nested ini files. Nested ini files look exactly like regular ini files, but allow for nested namespaces. For instance, the following code example [ namespace ] subnamespace . one = 1 subnamespace . two = 2 subnamespace . three = 3 is identical to [ namespace . subnamespace ] one = 1 two = 2 three = 3 Although this disallows '.' in identifiers, this is practical when information needs to be grouped in certain ways. For instance, loading additional parts and objects into scenarios makes use of nested namespaces for documentation purposes. You might want to add a certain part or object from a blender file such that you can specify textures or material properties directly within blender. However, your downstream application such as a deep network for pose estimation only understands PLY files and does not care about material properties. However, PLY files often use a scaling, e.g. in mm, that is different from what you might use in blender, e.g. m. To group all of this information, we make use of the following canonical way to describe parts that we load from an ini file: [ parts ] partname = / path / to / partname . blend ply . partname = / path / to / partname . ply ply_scale . partname = 0.010 Note that some configuration files, such as the Workstation Scenarios configuration, allow additional/extended specifications. Details about these are described in their corresponding documentation. ATTENTION: when scaling objects, the final behavior might be different between loading objects from .blend or from .ply since the intrinsic scales might be different within the two files. Anyway, for scaling objects load from .blend files use the corresponding blend_scale.partname config tag. Similarly, loading from ply use the ply_scale.partname config tag. Important Notes . For this to work properly, make sure that your parts have the correct scale, as well as rigid object properties. In particular, do not forget to make (in the blender file) the object an active rigid object with appropriate weight and margins for sensitivity. Also, make sure that the object\u2019s center is approximately at its real-world physical center. Often, PLY models don\u2019t have the object center at the physical or geometric center of the object. To quickly change this in blender, select the object and, in object mode, go to the toolbar \u201cObject\u201d -> \u201cSet Origin To\u201d and select an appropriate variant. Afterwards, it is best to move the object to location 0, 0, 0. Note that for the provided objects we moved the center to the geometrical center, as this is the most common usage in downward applications such as neural networks. In the provided scenes, we currently use a default weight of 0.01kg for most (small) objects and a sensitivity margin of 0.0001m for numerical stability. Setting configuration paramters on the command line \u00b6 Each (documented) configuration parameter can be set on the commandline. This is useful if you want to briefly test a setting before rendering thousands of images. For instance the Base Configuration argument dataset.image_count , which informs about how many images ABR shall render, can be set on the command line by $ abrgen --dataset.image_count 2 Equivalently, by using the explicit command $ blender -b -P path_to_abr_src/cli/render_dataset.py -- --dataset.image_count 2","title":"Overview"},{"location":"configs/overview/#overview","text":"","title":"Overview"},{"location":"configs/overview/#nested-ini-files","text":"The ABR pipeline is configured using nested ini files. Nested ini files look exactly like regular ini files, but allow for nested namespaces. For instance, the following code example [ namespace ] subnamespace . one = 1 subnamespace . two = 2 subnamespace . three = 3 is identical to [ namespace . subnamespace ] one = 1 two = 2 three = 3 Although this disallows '.' in identifiers, this is practical when information needs to be grouped in certain ways. For instance, loading additional parts and objects into scenarios makes use of nested namespaces for documentation purposes. You might want to add a certain part or object from a blender file such that you can specify textures or material properties directly within blender. However, your downstream application such as a deep network for pose estimation only understands PLY files and does not care about material properties. However, PLY files often use a scaling, e.g. in mm, that is different from what you might use in blender, e.g. m. To group all of this information, we make use of the following canonical way to describe parts that we load from an ini file: [ parts ] partname = / path / to / partname . blend ply . partname = / path / to / partname . ply ply_scale . partname = 0.010 Note that some configuration files, such as the Workstation Scenarios configuration, allow additional/extended specifications. Details about these are described in their corresponding documentation. ATTENTION: when scaling objects, the final behavior might be different between loading objects from .blend or from .ply since the intrinsic scales might be different within the two files. Anyway, for scaling objects load from .blend files use the corresponding blend_scale.partname config tag. Similarly, loading from ply use the ply_scale.partname config tag. Important Notes . For this to work properly, make sure that your parts have the correct scale, as well as rigid object properties. In particular, do not forget to make (in the blender file) the object an active rigid object with appropriate weight and margins for sensitivity. Also, make sure that the object\u2019s center is approximately at its real-world physical center. Often, PLY models don\u2019t have the object center at the physical or geometric center of the object. To quickly change this in blender, select the object and, in object mode, go to the toolbar \u201cObject\u201d -> \u201cSet Origin To\u201d and select an appropriate variant. Afterwards, it is best to move the object to location 0, 0, 0. Note that for the provided objects we moved the center to the geometrical center, as this is the most common usage in downward applications such as neural networks. In the provided scenes, we currently use a default weight of 0.01kg for most (small) objects and a sensitivity margin of 0.0001m for numerical stability.","title":"Nested INI Files"},{"location":"configs/overview/#setting-configuration-paramters-on-the-command-line","text":"Each (documented) configuration parameter can be set on the commandline. This is useful if you want to briefly test a setting before rendering thousands of images. For instance the Base Configuration argument dataset.image_count , which informs about how many images ABR shall render, can be set on the command line by $ abrgen --dataset.image_count 2 Equivalently, by using the explicit command $ blender -b -P path_to_abr_src/cli/render_dataset.py -- --dataset.image_count 2","title":"Setting configuration paramters on the command line"},{"location":"configs/pandatable/","text":"PandaTable Scenario \u00b6 The pandatable scenarion has the following options that you can set in a corresponding configuration file. For an example of configuration files that were used, have a look at config/pandatable_example.cfg . [ dataset ] # Specify how many images should be rendered image_count = 5 # Specify the base path where data will be written to. base_path = $ OUTDIR / PandaTable - Train # specify the scene type scene_type = PandaTable [ camera_info ] # In this section you specify the camera information, which will have a direct # impact on rendering results. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy # Where the fx, fy values represented focal lengths, and cx, cy defines the # camera's principal point. # # You can extract fx, fy, cx, cy from a calibration matrix K: # # fx s cx # K = 0 fy cy # 0 0 1 # # Note, however, that the values in your calibration matrix or intrinsics # specification might not end up in proper render resolutions. For instance, # this is the case in the example below, which would result in a rendering # resolution of about 1320.98 x 728.08. Blender will round these values to # suitable integer values. As a consequence, even if you set width and height # above to 0, the effective intrinsics that blender uses might be slightly # different from your K. # # To accomodate this 'issue', amira_blender_rendering will write a value # 'effective_intrinsics' to the configuration as soon as setting up cameras and # rendering is done. Recall that all configurations will be stored alongside the # created dataset, so you can easily retrieve the effective_intrinsics in # downstream applications intrinsics = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # zeroing angles rx, ry, rz in deg to account for camera non-zero default rotation zeroing = 0 , 0 , 0 [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 64 # allow occlusions of target objects (true, false) allow_occlusions = False [ scene_setup ] # specify the blender file from which to load the scene blend_file = $ DATA_GFX / modeling / robottable_empty . blend # specify where background / environment images will be taken from during # rendering. This can be a single file, or a directory containing images environment_texture = $ DATA / OpenImagesV4 / Images # specify which cameras to use for rendering. The names here follow the names in # the blender file, i.e. Camera, StereoCamera.Left, StereoCamera.Right cameras = Camera # cameras = Camera, StereoCamera.Left, StereoCamera.Right # number of frames to forward-simulate in the physics simulation forward_frames = 15 [ parts ] # This section allows you to add parts from separate blender or PLY files. There # are three different ways for specification # # 1) blender only # you need to specify a name of an object, and a blender file in # which the object resides in the format # part_name = blend_file # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # # Note: If no further configs are set, the object name *must* correspond # to the name that the object has in the blender file. # They will be loaded on-demand when setting up the scenario. # Loading objects from the same .blend file but with different names is # possible by using the `name.part_name` tag. # This might be useful in case you want to load the same object but with # different scale factors (see below for the use of blend_scale). # # Example: # my_cool_name = $DATA_GFX/cad/rexroth/hammerschraube.blend # name.my_cool_name = hammerschraube # # The `name.part_name` tag *must* correspond to the name the object has in the # blender file. After loading, the object name will be overwritten by `my_cool_name`. # # 2) blender + PLY # This variant is useful when you want to use the dataset later on and need # information about the origin of the blender model. # For instance, you might have access to a specific CAD model, and you want to # train a deep network to detect this CAD model. Such a network might require # more information from the CAD model to work. However, you probably do not # wish to load a blender file, but the (simpler) PLY file during network # training. Given that this configuration is stored alongside the generated # dataset, the information is in one place. # Note that, often, PLY CAD Models have a different scaling than blender # models. While blender defaults to using 1m, CAD software often defaults to # using mm or cm. Hence, you also need to specify a scale factor # # The format to specify the ply-file and scale factor is: # ply.part_name = path/to/ply # ply_scale.part_name = 1.0, 1.0, 1.0 # # Where the scale is a vector, consisting of the scaling in X, Y, and Z # dimensions. # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # ply_scale.hammerschraube = 0.001, 0.001, 0.001 # # However we also allow to scale objects loaded directly from .blend files. # For this, use the correpsonding `blend_scale.part_name` config tag. # # 3) PLY only # In case you only have access to a PLY file, you can specify everything # according to the aforementioned items but leave the blender path empty. # # Example: # hammerschraube = # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # scale.hammerschraube = 0.001, 0.001, 0.001 # # Important: Do *not* forget to add 'part_name =', despite not giving a # blender path name. This name will be required if you want to specify the # target_objects below # # Note: Make sure that in your blender files the parts are active rigid objects with # proper weight and sensitivity margin! # # Note/Attenton: We will not automatically add rigid body dynamics to ply-only models! # This means that if not actively added, the object will (by default) be # regarded as passive object (i.e., w/o rigid-body properties), hence not # subject to the dynamic simulation. # # ATTENTION: when scaling objects the final behavior might be different between # loading objects from .blend or from .ply since the intrinsic scales might # be different within the two files. # The first example is a \"hammerschraube\" (hammer head screw) hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . blend ply . hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . ply ply_scale . hammerschraube = 0.001 # The second example is a 60x60 angle element. winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . blend ply . winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . ply ply_scale . winkel_60x60 = 0.001 # this is a star knob sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . blend ply . sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . ply ply_scale . sterngriff = 0.001 # a cube-like connection wuerfelverbinder_40x40 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40 . blend ply . wuerfelverbinder_40x0 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40_3 . ply ply_scale . wuerfelverbinder_40x40 = 0.001 # a flanged nut bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend ply . bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8 = 0.001 # it is also possible to load objects from the same blend file # but using a different class name. This will be treated as different # objects in the annotations. Useful for e.g., loading same objects # with different scales bundmutter_m8_A = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend name . bundmutter_m8_A = bundmutter_m8 blend_scale . bundmutter_m8_A = 0.7 # similarly we can do with ply files. In this case, it is not # necessary to define a source name with the `name` tag since # when loading from PLY we are not binded to object names bundmutter_m8_B = ply . bundmutter_m8_B = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8_B = 0.003 # object 01 from the T-Less dataset tless_obj_01 = $ DATA_GFX / cad / tless / blender / obj_01 . blend ply . tless_obj_01 = $ DATA_GFX / cad / tless / models / obj_01 . ply ply_scale . tless_obj_01 = 0.001 # object 06 from the T-Less dataset tless_obj_06 = $ DATA_GFX / cad / tless / blender / obj_06 . blend ply . tless_obj_06 = $ DATA_GFX / cad / tless / models / obj_06 . ply ply_scale . tless_obj_06 = 0.001 # object 06 from the T-Less dataset tless_obj_13 = $ DATA_GFX / cad / tless / blender / obj_13 . blend ply . tless_obj_13 = $ DATA_GFX / cad / tless / models / obj_13 . ply ply_scale . tless_obj_13 = 0.001 [ scenario_setup ] # Specify all target objects that shall be dropped at random locations into the # environment. Target objects are all those objects that are already in the # .blend file in the 'Proto' collection. You can also specify parts that were # presented above using the syntax 'parts.partname:count' target_objects = parts . sterngriff : 4 , parts . wuerfelverbinder_40x40 : 3 , parts . hammerschraube : 7 , parts . winkel_60x60 : 5 # Similarly we allow to select additional objects to drop in the environment for which # annotated information are NOT stored, i.e., they serve as distractors distractor_objects = [] # Camera multiview is applied to all cameras selected in scene_setup.cameras and # it is activated calling abrgen with the --render-mode multiview flag. # For specific multiview modes/configs config refer to \"Multiview Configuration\" docs. [ multiview_setup ] # control how multiview camera locations are generated (bezier curve, circle, viewsphere etc.) mode = # mode specific configuration mode_config = # additional debug configs (used is debug.enable=True) [ debug ] # if in debug mode (see baseconfiguration), produce temporary plot of camera locations (best for multiview rendering) plot = False # if in debug mode (see baseconfiguration), plot coordinate axis system for camera # poses in before multiview rendering plot_axis = False # if in debug mode (see baseconfiguration), toggle scatter plot of camera locations # before multiview rendering scatter = False # if in debug mode (see baseconfiguration), enable saving to blender. # The option can be used to e.g., inspect whether multiple camera locations are occluded, # check object occlusions, check the dymanic simulation. save_to_blend = False","title":"Panda Table"},{"location":"configs/pandatable/#pandatable-scenario","text":"The pandatable scenarion has the following options that you can set in a corresponding configuration file. For an example of configuration files that were used, have a look at config/pandatable_example.cfg . [ dataset ] # Specify how many images should be rendered image_count = 5 # Specify the base path where data will be written to. base_path = $ OUTDIR / PandaTable - Train # specify the scene type scene_type = PandaTable [ camera_info ] # In this section you specify the camera information, which will have a direct # impact on rendering results. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy # Where the fx, fy values represented focal lengths, and cx, cy defines the # camera's principal point. # # You can extract fx, fy, cx, cy from a calibration matrix K: # # fx s cx # K = 0 fy cy # 0 0 1 # # Note, however, that the values in your calibration matrix or intrinsics # specification might not end up in proper render resolutions. For instance, # this is the case in the example below, which would result in a rendering # resolution of about 1320.98 x 728.08. Blender will round these values to # suitable integer values. As a consequence, even if you set width and height # above to 0, the effective intrinsics that blender uses might be slightly # different from your K. # # To accomodate this 'issue', amira_blender_rendering will write a value # 'effective_intrinsics' to the configuration as soon as setting up cameras and # rendering is done. Recall that all configurations will be stored alongside the # created dataset, so you can easily retrieve the effective_intrinsics in # downstream applications intrinsics = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # zeroing angles rx, ry, rz in deg to account for camera non-zero default rotation zeroing = 0 , 0 , 0 [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 64 # allow occlusions of target objects (true, false) allow_occlusions = False [ scene_setup ] # specify the blender file from which to load the scene blend_file = $ DATA_GFX / modeling / robottable_empty . blend # specify where background / environment images will be taken from during # rendering. This can be a single file, or a directory containing images environment_texture = $ DATA / OpenImagesV4 / Images # specify which cameras to use for rendering. The names here follow the names in # the blender file, i.e. Camera, StereoCamera.Left, StereoCamera.Right cameras = Camera # cameras = Camera, StereoCamera.Left, StereoCamera.Right # number of frames to forward-simulate in the physics simulation forward_frames = 15 [ parts ] # This section allows you to add parts from separate blender or PLY files. There # are three different ways for specification # # 1) blender only # you need to specify a name of an object, and a blender file in # which the object resides in the format # part_name = blend_file # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # # Note: If no further configs are set, the object name *must* correspond # to the name that the object has in the blender file. # They will be loaded on-demand when setting up the scenario. # Loading objects from the same .blend file but with different names is # possible by using the `name.part_name` tag. # This might be useful in case you want to load the same object but with # different scale factors (see below for the use of blend_scale). # # Example: # my_cool_name = $DATA_GFX/cad/rexroth/hammerschraube.blend # name.my_cool_name = hammerschraube # # The `name.part_name` tag *must* correspond to the name the object has in the # blender file. After loading, the object name will be overwritten by `my_cool_name`. # # 2) blender + PLY # This variant is useful when you want to use the dataset later on and need # information about the origin of the blender model. # For instance, you might have access to a specific CAD model, and you want to # train a deep network to detect this CAD model. Such a network might require # more information from the CAD model to work. However, you probably do not # wish to load a blender file, but the (simpler) PLY file during network # training. Given that this configuration is stored alongside the generated # dataset, the information is in one place. # Note that, often, PLY CAD Models have a different scaling than blender # models. While blender defaults to using 1m, CAD software often defaults to # using mm or cm. Hence, you also need to specify a scale factor # # The format to specify the ply-file and scale factor is: # ply.part_name = path/to/ply # ply_scale.part_name = 1.0, 1.0, 1.0 # # Where the scale is a vector, consisting of the scaling in X, Y, and Z # dimensions. # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # ply_scale.hammerschraube = 0.001, 0.001, 0.001 # # However we also allow to scale objects loaded directly from .blend files. # For this, use the correpsonding `blend_scale.part_name` config tag. # # 3) PLY only # In case you only have access to a PLY file, you can specify everything # according to the aforementioned items but leave the blender path empty. # # Example: # hammerschraube = # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # scale.hammerschraube = 0.001, 0.001, 0.001 # # Important: Do *not* forget to add 'part_name =', despite not giving a # blender path name. This name will be required if you want to specify the # target_objects below # # Note: Make sure that in your blender files the parts are active rigid objects with # proper weight and sensitivity margin! # # Note/Attenton: We will not automatically add rigid body dynamics to ply-only models! # This means that if not actively added, the object will (by default) be # regarded as passive object (i.e., w/o rigid-body properties), hence not # subject to the dynamic simulation. # # ATTENTION: when scaling objects the final behavior might be different between # loading objects from .blend or from .ply since the intrinsic scales might # be different within the two files. # The first example is a \"hammerschraube\" (hammer head screw) hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . blend ply . hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . ply ply_scale . hammerschraube = 0.001 # The second example is a 60x60 angle element. winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . blend ply . winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . ply ply_scale . winkel_60x60 = 0.001 # this is a star knob sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . blend ply . sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . ply ply_scale . sterngriff = 0.001 # a cube-like connection wuerfelverbinder_40x40 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40 . blend ply . wuerfelverbinder_40x0 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40_3 . ply ply_scale . wuerfelverbinder_40x40 = 0.001 # a flanged nut bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend ply . bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8 = 0.001 # it is also possible to load objects from the same blend file # but using a different class name. This will be treated as different # objects in the annotations. Useful for e.g., loading same objects # with different scales bundmutter_m8_A = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend name . bundmutter_m8_A = bundmutter_m8 blend_scale . bundmutter_m8_A = 0.7 # similarly we can do with ply files. In this case, it is not # necessary to define a source name with the `name` tag since # when loading from PLY we are not binded to object names bundmutter_m8_B = ply . bundmutter_m8_B = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8_B = 0.003 # object 01 from the T-Less dataset tless_obj_01 = $ DATA_GFX / cad / tless / blender / obj_01 . blend ply . tless_obj_01 = $ DATA_GFX / cad / tless / models / obj_01 . ply ply_scale . tless_obj_01 = 0.001 # object 06 from the T-Less dataset tless_obj_06 = $ DATA_GFX / cad / tless / blender / obj_06 . blend ply . tless_obj_06 = $ DATA_GFX / cad / tless / models / obj_06 . ply ply_scale . tless_obj_06 = 0.001 # object 06 from the T-Less dataset tless_obj_13 = $ DATA_GFX / cad / tless / blender / obj_13 . blend ply . tless_obj_13 = $ DATA_GFX / cad / tless / models / obj_13 . ply ply_scale . tless_obj_13 = 0.001 [ scenario_setup ] # Specify all target objects that shall be dropped at random locations into the # environment. Target objects are all those objects that are already in the # .blend file in the 'Proto' collection. You can also specify parts that were # presented above using the syntax 'parts.partname:count' target_objects = parts . sterngriff : 4 , parts . wuerfelverbinder_40x40 : 3 , parts . hammerschraube : 7 , parts . winkel_60x60 : 5 # Similarly we allow to select additional objects to drop in the environment for which # annotated information are NOT stored, i.e., they serve as distractors distractor_objects = [] # Camera multiview is applied to all cameras selected in scene_setup.cameras and # it is activated calling abrgen with the --render-mode multiview flag. # For specific multiview modes/configs config refer to \"Multiview Configuration\" docs. [ multiview_setup ] # control how multiview camera locations are generated (bezier curve, circle, viewsphere etc.) mode = # mode specific configuration mode_config = # additional debug configs (used is debug.enable=True) [ debug ] # if in debug mode (see baseconfiguration), produce temporary plot of camera locations (best for multiview rendering) plot = False # if in debug mode (see baseconfiguration), plot coordinate axis system for camera # poses in before multiview rendering plot_axis = False # if in debug mode (see baseconfiguration), toggle scatter plot of camera locations # before multiview rendering scatter = False # if in debug mode (see baseconfiguration), enable saving to blender. # The option can be used to e.g., inspect whether multiple camera locations are occluded, # check object occlusions, check the dymanic simulation. save_to_blend = False","title":"PandaTable Scenario"},{"location":"configs/simpleobject/","text":"Simple Object Scenario \u00b6 The Simple Object Scenario represent a prototypical scenario of the simplest type implemented in ABR. For an more in depth tutorial on how to set up a very simple custom scenario refer to our tutorial on how to set up a simple custom scenario Most of the following configurations are common to other scenarios. Hence, please refer to Base Configuration for an overview. For the SimpleObject scenario refer to the config file config/examples/single_object_toolcap_example.cfg [ dataset ] # In this section we specify basic properties of the dataset # Specify how many images should be rendered image_count = 5 # Specify the base path where data will be written to. Note that, # differently to more complex scenarios, e.g., see :ref:`Workstation Scenarios`, # here there is only one configuration and one camera. # Hence hence the base path coincides with the location where data are actually written. base_path = $ OUTDIR / SimpleToolCap - Train # specify the scene type scene_type = SimpleObject [ camera_info ] # In this section we specify camera specific configurations. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy intrinsic = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # zeroing angles rx, ry, rz in deg to account for camera non-zero default rotation zeroing = 0 , 0 , 0 [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 8 [ scene_setup ] # we also specify where to load environment textures from # # For this simple case, this is the only scene-specific configuration value environment_textures = $ DATA / OpenImagesV4 / Images [ parts ] # here we use the 'ply only' version to load objects. Fore more documentation, # see config/workstation_scenario01_test.cfg ToolCap = # this scene loads a tool cap mesh. This is loaded from the corresponding mesh ply . ToolCap = $ DATA_GFX / cad / parts / tool_cap_x10 . ply # ply models often have a different scale than what is used in blender. Here, we # have to scale down the model to match blender units (which are treated to be # meters) ply_scale . ToolCap = 0.001 , 0.001 , 0.001 # another single object to try LetterB = ply . LetterB = $ DATA_GFX / cad / parts / B . ply ply_scale . LetterB = 0.001 , 0.001 , 0.001 [ scenario_setup ] # here we specify the objects of interest. In the case of this demo, we are only # interested in one part of type \"tool_cap\". Although this configuration option # is not used in the backend script, it is useful to document the items that are # part of the scenario target_object = ToolCap object_material = metal","title":"Simple Object"},{"location":"configs/simpleobject/#simple-object-scenario","text":"The Simple Object Scenario represent a prototypical scenario of the simplest type implemented in ABR. For an more in depth tutorial on how to set up a very simple custom scenario refer to our tutorial on how to set up a simple custom scenario Most of the following configurations are common to other scenarios. Hence, please refer to Base Configuration for an overview. For the SimpleObject scenario refer to the config file config/examples/single_object_toolcap_example.cfg [ dataset ] # In this section we specify basic properties of the dataset # Specify how many images should be rendered image_count = 5 # Specify the base path where data will be written to. Note that, # differently to more complex scenarios, e.g., see :ref:`Workstation Scenarios`, # here there is only one configuration and one camera. # Hence hence the base path coincides with the location where data are actually written. base_path = $ OUTDIR / SimpleToolCap - Train # specify the scene type scene_type = SimpleObject [ camera_info ] # In this section we specify camera specific configurations. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy intrinsic = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # zeroing angles rx, ry, rz in deg to account for camera non-zero default rotation zeroing = 0 , 0 , 0 [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 8 [ scene_setup ] # we also specify where to load environment textures from # # For this simple case, this is the only scene-specific configuration value environment_textures = $ DATA / OpenImagesV4 / Images [ parts ] # here we use the 'ply only' version to load objects. Fore more documentation, # see config/workstation_scenario01_test.cfg ToolCap = # this scene loads a tool cap mesh. This is loaded from the corresponding mesh ply . ToolCap = $ DATA_GFX / cad / parts / tool_cap_x10 . ply # ply models often have a different scale than what is used in blender. Here, we # have to scale down the model to match blender units (which are treated to be # meters) ply_scale . ToolCap = 0.001 , 0.001 , 0.001 # another single object to try LetterB = ply . LetterB = $ DATA_GFX / cad / parts / B . ply ply_scale . LetterB = 0.001 , 0.001 , 0.001 [ scenario_setup ] # here we specify the objects of interest. In the case of this demo, we are only # interested in one part of type \"tool_cap\". Although this configuration option # is not used in the backend script, it is useful to document the items that are # part of the scenario target_object = ToolCap object_material = metal","title":"Simple Object Scenario"},{"location":"configs/workstation_scenarios/","text":"Workstation Scenarios \u00b6 The workstation scenarios have the following options that you can set in a corresponding configuration file. For an example of configuration files that were used, have a look at config/examples/workstation_scenarios*.cfg . [ dataset ] # Specify how many images should be rendered image_count = 5 # Specify the base path where data will be written to. Note that this is a base # path, to which additional information will be added such as Scenario-Number # and Camera-Name base_path = $ OUTDIR / WorkstationScenarios - Train # specify the scene type scene_type = WorkstationScenarios [ camera_info ] # In this section you specify the camera information, which will have a direct # impact on rendering results. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy # Where the fx, fy values represented focal lengths, and cx, cy defines the # camera's principal point. # # You can extract fx, fy, cx, cy from a calibration matrix K: # # fx s cx # K = 0 fy cy # 0 0 1 # # Note, however, that the values in your calibration matrix or intrinsics # specification might not end up in proper render resolutions. For instance, # this is the case in the example below, which would result in a rendering # resolution of about 1320.98 x 728.08. Blender will round these values to # suitable integer values. As a consequence, even if you set width and height # above to 0, the effective intrinsics that blender uses might be slightly # different from your K. # # To accomodate this 'issue', amira_blender_rendering will write a value # 'effective_intrinsics' to the configuration as soon as setting up cameras and # rendering is done. Recall that all configurations will be stored alongside the # created dataset, so you can easily retrieve the effective_intrinsics in # downstream applications intrinsics = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # zeroing angles rx, ry, rz in deg to account for camera non-zero default rotation zeroing = 0 , 0 , 0 [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 64 # allow occlusions of target objects (true, false) allow_occlusions = False [ scene_setup ] # specify the blender file from which to load the scene blend_file = $ DATA_GFX / modeling / workstation_scenarios . blend # specify where background / environment images will be taken from during # rendering. This can be a single file, or a directory containing images environment_texture = $ DATA / OpenImagesV4 / Images # specify which cameras to use for rendering. The names here follow the names in # the blender file, i.e. Camera, StereoCamera.Left, StereoCamera.Right cameras = Camera # cameras = Camera, StereoCamera.Left, StereoCamera.Right # number of frames to forward-simulate in the physics simulation forward_frames = 15 [ parts ] # This section allows you to add parts from separate blender or PLY files. There # are three different ways for specification # # 1) blender only # you need to specify a name of an object, and a blender file in # which the object resides in the format # part_name = blend_file # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # # Note: If no further configs are set, the object name *must* correspond # to the name that the object has in the blender file. # They will be loaded on-demand when setting up the scenario. # Loading objects from the same .blend file but with different names is # possible by using the `name.part_name` tag. # This might be useful in case you want to load the same object but with # different scale factors (see below for the use of blend_scale). # # Example: # my_cool_name = $DATA_GFX/cad/rexroth/hammerschraube.blend # name.my_cool_name = hammerschraube # # The `name.part_name` tag *must* correspond to the name the object has in the # blender file. After loading, the object name will be overwritten by `my_cool_name`. # # 2) blender + PLY # This variant is useful when you want to use the dataset later on and need # information about the origin of the blender model. # For instance, you might have access to a specific CAD model, and you want to # train a deep network to detect this CAD model. Such a network might require # more information from the CAD model to work. However, you probably do not # wish to load a blender file, but the (simpler) PLY file during network # training. Given that this configuration is stored alongside the generated # dataset, the information is in one place. # Note that, often, PLY CAD Models have a different scaling than blender # models. While blender defaults to using 1m, CAD software often defaults to # using mm or cm. Hence, you also need to specify a scale factor # # The format to specify the ply-file and scale factor is: # ply.part_name = path/to/ply # ply_scale.part_name = 1.0, 1.0, 1.0 # # Where the scale is a vector, consisting of the scaling in X, Y, and Z # dimensions. # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # ply_scale.hammerschraube = 0.001, 0.001, 0.001 # # However we also allow to scale objects loaded directly from .blend files. # For this, use the correpsonding `blend_scale.part_name` config tag. # # 3) PLY only # In case you only have access to a PLY file, you can specify everything # according to the aforementioned items but leave the blender path empty. # # Example: # hammerschraube = # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # scale.hammerschraube = 0.001, 0.001, 0.001 # # Important: Do *not* forget to add 'part_name =', despite not giving a # blender path name. This name will be required if you want to specify the # target_objects below # # Note: Make sure that in your blender files the parts are active rigid objects with # proper weight and sensitivity margin! # # Note/Attenton: We will not automatically add rigid body dynamics to ply-only models! # This means that if not actively added, the object will (by default) be # regarded as passive object (i.e., w/o rigid-body properties), hence not # subject to the dynamic simulation. # # ATTENTION: when scaling objects the final behavior might be different between # loading objects from .blend or from .ply since the intrinsic scales might # be different within the two files. # The first example is a \"hammerschraube\" (hammer head screw) hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . blend ply . hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . ply ply_scale . hammerschraube = 0.001 # The second example is a 60x60 angle element. winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . blend ply . winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . ply ply_scale . winkel_60x60 = 0.001 # this is a star knob sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . blend ply . sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . ply ply_scale . sterngriff = 0.001 # a cube-like connection wuerfelverbinder_40x40 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40 . blend ply . wuerfelverbinder_40x0 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40_3 . ply ply_scale . wuerfelverbinder_40x40 = 0.001 # a flanged nut bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend ply . bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8 = 0.001 # it is also possible to load objects from the same blend file # but using a different class name. This will be treated as different # objects in the annotations. Useful for e.g., loading same objects # with different scales bundmutter_m8_A = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend name . bundmutter_m8_A = bundmutter_m8 blend_scale . bundmutter_m8_A = 0.7 # similarly we can do with ply files. In this case, it is not # necessary to define a source name with the `name` tag since # when loading from PLY we are not binded to object names. bundmutter_m8_B = ply . bundmutter_m8_B = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8_B = 0.003 # object 01 from the T-Less dataset tless_obj_01 = $ DATA_GFX / cad / tless / blender / obj_01 . blend ply . tless_obj_01 = $ DATA_GFX / cad / tless / models / obj_01 . ply ply_scale . tless_obj_01 = 0.001 # object 06 from the T-Less dataset tless_obj_06 = $ DATA_GFX / cad / tless / blender / obj_06 . blend ply . tless_obj_06 = $ DATA_GFX / cad / tless / models / obj_06 . ply ply_scale . tless_obj_06 = 0.001 # object 06 from the T-Less dataset tless_obj_13 = $ DATA_GFX / cad / tless / blender / obj_13 . blend ply . tless_obj_13 = $ DATA_GFX / cad / tless / models / obj_13 . ply ply_scale . tless_obj_13 = 0.001 [ scenario_setup ] # At the moment, the 6 different scenarios in workstation_scenarios.blend are # simply enumerated. Have a look at the .blend file for the order in which they # appear, e.g. identifiable by the numbering of the cameras scenario = 1 # Specify all target objects that shall be dropped at random locations into the # environment. Target objects are all those objects that are already in the # .blend file in the 'Proto' collection. You can also specify parts that were # presented above using the syntax 'parts.partname:count' target_objects = parts . sterngriff : 4 , parts . wuerfelverbinder_40x40 : 3 , parts . hammerschraube : 7 , parts . winkel_60x60 : 5 # Also we allow to select and set of objects to be dropped in the scene but # of which annotated information are NOT stored, i.e., they serve as distractors distractor_objects = parts . tless_obj_06 : 3 # Finally, similarly to target objects, specify the list of ABC objects to load abc_objects = # Specify number of random metallic materials to generate for ABC objects abc_color_count = 3 # Camera multiview is applied to all cameras selected in scene_setup.cameras and # it is activated calling abrgen with the --render-mode multiview flag. # For specific multiview modes/configs config refer to \"Multiview Configuration\" docs. [ multiview_setup ] # control how multiview camera locations are generated (bezier curve, circle, viewsphere etc.) mode = # mode specific configuration mode_config = # additional debug configs (used is debug.enable=True) [ debug ] # if in debug mode (see baseconfiguration), produce temporary plot of camera locations (best for multiview rendering) plot = False # if in debug mode (see baseconfiguration), plot coordinate axis system for camera # poses in before multiview rendering plot_axis = False # if in debug mode (see baseconfiguration), toggle scatter plot of camera locations # before multiview rendering scatter = False # if in debug mode (see baseconfiguration), enable saving to blender. # The option can be used to e.g., inspect whether multiple camera locations are occluded, # check object occlusions, check the dymanic simulation. save_to_blend = False","title":"Workstation"},{"location":"configs/workstation_scenarios/#workstation-scenarios","text":"The workstation scenarios have the following options that you can set in a corresponding configuration file. For an example of configuration files that were used, have a look at config/examples/workstation_scenarios*.cfg . [ dataset ] # Specify how many images should be rendered image_count = 5 # Specify the base path where data will be written to. Note that this is a base # path, to which additional information will be added such as Scenario-Number # and Camera-Name base_path = $ OUTDIR / WorkstationScenarios - Train # specify the scene type scene_type = WorkstationScenarios [ camera_info ] # In this section you specify the camera information, which will have a direct # impact on rendering results. # The width and height have an influence on the rendering resolution. In case # you wish to set a specific calibration matrix that you obtained, for # instance, from OpenCV, and do not wish to temper with the rendering # resolution, then set these values to 0. width = 640 height = 480 # The camera model to use. At the moment, this value is ignored in # amira_blender_rendering. However, because all rendering is in fact done with a # pinhole camera model, this value serves as documentation model = pinhole # Also this value has no impact on rendering likewise the model. However, if # you want to specify a certain camera name for documentation purposes, this is # the place. name = Pinhole Camera # You can specify the intrinsic calibration information that was determined for # a camera, for instance with OpenCV. # # Here, we use the format # intrinsics = fx, fy, cx, cy # Where the fx, fy values represented focal lengths, and cx, cy defines the # camera's principal point. # # You can extract fx, fy, cx, cy from a calibration matrix K: # # fx s cx # K = 0 fy cy # 0 0 1 # # Note, however, that the values in your calibration matrix or intrinsics # specification might not end up in proper render resolutions. For instance, # this is the case in the example below, which would result in a rendering # resolution of about 1320.98 x 728.08. Blender will round these values to # suitable integer values. As a consequence, even if you set width and height # above to 0, the effective intrinsics that blender uses might be slightly # different from your K. # # To accomodate this 'issue', amira_blender_rendering will write a value # 'effective_intrinsics' to the configuration as soon as setting up cameras and # rendering is done. Recall that all configurations will be stored alongside the # created dataset, so you can easily retrieve the effective_intrinsics in # downstream applications intrinsics = 9.9801747708520452e+02 , 9.9264009290521165e+02 , 6.6049856967197002e+02 , 3.6404286361152555e+02 , 0 # zeroing angles rx, ry, rz in deg to account for camera non-zero default rotation zeroing = 0 , 0 , 0 [ render_setup ] # specify which renderer to use. Usually you should leave this at # blender-cycles. Note that, at the moment, this is hard-coded to cycles # internally anyway. backend = blender - cycles # integrator (either PATH or BRANCHED_PATH) integrator = BRANCHED_PATH # use denoising (true, false) denoising = True # samples the ray-tracer uses per pixel samples = 64 # allow occlusions of target objects (true, false) allow_occlusions = False [ scene_setup ] # specify the blender file from which to load the scene blend_file = $ DATA_GFX / modeling / workstation_scenarios . blend # specify where background / environment images will be taken from during # rendering. This can be a single file, or a directory containing images environment_texture = $ DATA / OpenImagesV4 / Images # specify which cameras to use for rendering. The names here follow the names in # the blender file, i.e. Camera, StereoCamera.Left, StereoCamera.Right cameras = Camera # cameras = Camera, StereoCamera.Left, StereoCamera.Right # number of frames to forward-simulate in the physics simulation forward_frames = 15 [ parts ] # This section allows you to add parts from separate blender or PLY files. There # are three different ways for specification # # 1) blender only # you need to specify a name of an object, and a blender file in # which the object resides in the format # part_name = blend_file # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # # Note: If no further configs are set, the object name *must* correspond # to the name that the object has in the blender file. # They will be loaded on-demand when setting up the scenario. # Loading objects from the same .blend file but with different names is # possible by using the `name.part_name` tag. # This might be useful in case you want to load the same object but with # different scale factors (see below for the use of blend_scale). # # Example: # my_cool_name = $DATA_GFX/cad/rexroth/hammerschraube.blend # name.my_cool_name = hammerschraube # # The `name.part_name` tag *must* correspond to the name the object has in the # blender file. After loading, the object name will be overwritten by `my_cool_name`. # # 2) blender + PLY # This variant is useful when you want to use the dataset later on and need # information about the origin of the blender model. # For instance, you might have access to a specific CAD model, and you want to # train a deep network to detect this CAD model. Such a network might require # more information from the CAD model to work. However, you probably do not # wish to load a blender file, but the (simpler) PLY file during network # training. Given that this configuration is stored alongside the generated # dataset, the information is in one place. # Note that, often, PLY CAD Models have a different scaling than blender # models. While blender defaults to using 1m, CAD software often defaults to # using mm or cm. Hence, you also need to specify a scale factor # # The format to specify the ply-file and scale factor is: # ply.part_name = path/to/ply # ply_scale.part_name = 1.0, 1.0, 1.0 # # Where the scale is a vector, consisting of the scaling in X, Y, and Z # dimensions. # # Example: # hammerschraube = $DATA_GFX/cad/rexroth/hammerschraube.blend # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # ply_scale.hammerschraube = 0.001, 0.001, 0.001 # # However we also allow to scale objects loaded directly from .blend files. # For this, use the correpsonding `blend_scale.part_name` config tag. # # 3) PLY only # In case you only have access to a PLY file, you can specify everything # according to the aforementioned items but leave the blender path empty. # # Example: # hammerschraube = # ply.hammerschraube = $DATA_PERCEPTION/CADModels/rexroth/ # scale.hammerschraube = 0.001, 0.001, 0.001 # # Important: Do *not* forget to add 'part_name =', despite not giving a # blender path name. This name will be required if you want to specify the # target_objects below # # Note: Make sure that in your blender files the parts are active rigid objects with # proper weight and sensitivity margin! # # Note/Attenton: We will not automatically add rigid body dynamics to ply-only models! # This means that if not actively added, the object will (by default) be # regarded as passive object (i.e., w/o rigid-body properties), hence not # subject to the dynamic simulation. # # ATTENTION: when scaling objects the final behavior might be different between # loading objects from .blend or from .ply since the intrinsic scales might # be different within the two files. # The first example is a \"hammerschraube\" (hammer head screw) hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . blend ply . hammerschraube = $ DATA_GFX / cad / rexroth / hammerschraube . ply ply_scale . hammerschraube = 0.001 # The second example is a 60x60 angle element. winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . blend ply . winkel_60x60 = $ DATA_GFX / cad / rexroth / winkel_60x60 . ply ply_scale . winkel_60x60 = 0.001 # this is a star knob sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . blend ply . sterngriff = $ DATA_GFX / cad / rexroth / sterngriff . ply ply_scale . sterngriff = 0.001 # a cube-like connection wuerfelverbinder_40x40 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40 . blend ply . wuerfelverbinder_40x0 = $ DATA_GFX / cad / rexroth / wuerfelverbinder_40x40_3 . ply ply_scale . wuerfelverbinder_40x40 = 0.001 # a flanged nut bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend ply . bundmutter_m8 = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8 = 0.001 # it is also possible to load objects from the same blend file # but using a different class name. This will be treated as different # objects in the annotations. Useful for e.g., loading same objects # with different scales bundmutter_m8_A = $ DATA_GFX / cad / rexroth / bundmutter_m8 . blend name . bundmutter_m8_A = bundmutter_m8 blend_scale . bundmutter_m8_A = 0.7 # similarly we can do with ply files. In this case, it is not # necessary to define a source name with the `name` tag since # when loading from PLY we are not binded to object names. bundmutter_m8_B = ply . bundmutter_m8_B = $ DATA_GFX / cad / rexroth / bundmutter_m8 . ply ply_scale . bundmutter_m8_B = 0.003 # object 01 from the T-Less dataset tless_obj_01 = $ DATA_GFX / cad / tless / blender / obj_01 . blend ply . tless_obj_01 = $ DATA_GFX / cad / tless / models / obj_01 . ply ply_scale . tless_obj_01 = 0.001 # object 06 from the T-Less dataset tless_obj_06 = $ DATA_GFX / cad / tless / blender / obj_06 . blend ply . tless_obj_06 = $ DATA_GFX / cad / tless / models / obj_06 . ply ply_scale . tless_obj_06 = 0.001 # object 06 from the T-Less dataset tless_obj_13 = $ DATA_GFX / cad / tless / blender / obj_13 . blend ply . tless_obj_13 = $ DATA_GFX / cad / tless / models / obj_13 . ply ply_scale . tless_obj_13 = 0.001 [ scenario_setup ] # At the moment, the 6 different scenarios in workstation_scenarios.blend are # simply enumerated. Have a look at the .blend file for the order in which they # appear, e.g. identifiable by the numbering of the cameras scenario = 1 # Specify all target objects that shall be dropped at random locations into the # environment. Target objects are all those objects that are already in the # .blend file in the 'Proto' collection. You can also specify parts that were # presented above using the syntax 'parts.partname:count' target_objects = parts . sterngriff : 4 , parts . wuerfelverbinder_40x40 : 3 , parts . hammerschraube : 7 , parts . winkel_60x60 : 5 # Also we allow to select and set of objects to be dropped in the scene but # of which annotated information are NOT stored, i.e., they serve as distractors distractor_objects = parts . tless_obj_06 : 3 # Finally, similarly to target objects, specify the list of ABC objects to load abc_objects = # Specify number of random metallic materials to generate for ABC objects abc_color_count = 3 # Camera multiview is applied to all cameras selected in scene_setup.cameras and # it is activated calling abrgen with the --render-mode multiview flag. # For specific multiview modes/configs config refer to \"Multiview Configuration\" docs. [ multiview_setup ] # control how multiview camera locations are generated (bezier curve, circle, viewsphere etc.) mode = # mode specific configuration mode_config = # additional debug configs (used is debug.enable=True) [ debug ] # if in debug mode (see baseconfiguration), produce temporary plot of camera locations (best for multiview rendering) plot = False # if in debug mode (see baseconfiguration), plot coordinate axis system for camera # poses in before multiview rendering plot_axis = False # if in debug mode (see baseconfiguration), toggle scatter plot of camera locations # before multiview rendering scatter = False # if in debug mode (see baseconfiguration), enable saving to blender. # The option can be used to e.g., inspect whether multiple camera locations are occluded, # check object occlusions, check the dymanic simulation. save_to_blend = False","title":"Workstation Scenarios"},{"location":"tutorials/renderpredefinedscene/","text":"Basic - Render a predefined scene \u00b6 To get familiar with ABR, let\u2019s run and render a predefined scene. At the end of this tutorial you should expect to be able to successfully render a small dataset of images of a metal cap on random backgrounds. It is assumed ABR is already installed and tested, according to our installation If you are eager to see some images simply jump at the end to see how to commence the rendering. However, if this is your first time using ABR, we suggest you take a look and follow the entire tutotial step by step. Scene \u00b6 This tutorial is based on a predefined scene located in /src/amira_belender_rendering/scenes/simpleobject.py We suggest to take a look at it and try to understand it. The file can be used as prototypical example in order to set up other scenes. For a more in depth explanation refer to the following tutorial on set up a simple custom sceneario Assuming you are familiar with python, a few explanations are in order: the class SimpleObjectConfiguration is used to manage scene specific config values the class SimpleObject is used to control the logic of the desired scene. More specifically, at runtime, after setting up a class object, blender calls the generate_dataset which, in turn, takes care of: randomizing the environment (background) textures randomizing the pose of the target object (in this case a metal cap) adjust the file paths through the rendermanager calling the rendering performing some post processing steps def generate_dataset ( self ): # filename setup image_count = self . config . dataset . image_count if image_count <= 0 : return False format_width = int ( ceil ( log ( image_count , 10 ))) i = 0 while i < self . config . dataset . image_count : # generate render filename: adhere to naming convention base_filename = f \"s { i : 0 { format_width }} _v0\" # randomize environment and object transform self . randomize_environment_texture () self . randomize_object_transforms () # setup render managers' path specification self . renderman . setup_pathspec ( self . dirinfo , base_filename , self . objs ) # render the image self . renderman . render () # try to postprocess. This might fail, in which case we should # attempt to re-render the scene with different randomization try : self . renderman . postprocess ( self . dirinfo , base_filename , bpy . context . scene . camera , self . objs , self . config . camera_info . zeroing , postprocess_config = self . config . postprocess ) except ValueError : self . logger . warn ( \"ValueError during post-processing, re-generating image index {i} \" ) else : i = i + 1 return True Data \u00b6 Make sure the files B.ply and tool_cap_x10.ply are placed in /data/cad/parts . Also, as background textures we use images from Open Images a publicly available large dataset of various images. Make sure to download they images and store them locally, e.g., in a dedicated folder in $HOME/data/OpenImagesV4/Images . Note : In the tutorial we are going to render images of a metal cap with random background hence, we do not need any other data (such as a dedicated blender scene). Configuration file \u00b6 Take a look at the configuration file /config/examples/single_object_toolcap_example.cfg . The file is used to specifiy a bunch of configuration values for the dataset we are going to render such as the number of images to render etc. As you can see, we exploits some enviroment variables in order to parametrize some configuration values and make them agnostic user specific settings. In particular, these are: OUTDIR : path to folder where the rendered dataset will be stored DATA : path where backgrounds data are stored DATA_GFX : path where graphics data are stored In order to successfully render the dataset, you need to explictly set the above three variables. You can do that e.g., in you ~/.bashrc or inline as shown in the following section. Commence the rendering \u00b6 Now all that is left is to submit the rendering. It is assumed you have amira_blender_rendering repo available in your $HOME and that you set up the data as explained above . Then, to commence the rendering run OUTDIR = ~/data \\ DATA_GFX = ~/amira_blender_rendering/data \\ DATA = ~/data \\ abrgen --config ~/amira_blender_rendering/config/examples/single_object_toolcap_example.cfg In case you get Error: Could not import amira_blender_rendering. Either install it as a package, or specify a valid path to its location with the --abr-path command line argument. specify the --abr-path CLI argument as explained here .","title":"Basic - render a predefined scene"},{"location":"tutorials/renderpredefinedscene/#basic-render-a-predefined-scene","text":"To get familiar with ABR, let\u2019s run and render a predefined scene. At the end of this tutorial you should expect to be able to successfully render a small dataset of images of a metal cap on random backgrounds. It is assumed ABR is already installed and tested, according to our installation If you are eager to see some images simply jump at the end to see how to commence the rendering. However, if this is your first time using ABR, we suggest you take a look and follow the entire tutotial step by step.","title":"Basic - Render a predefined scene"},{"location":"tutorials/renderpredefinedscene/#scene","text":"This tutorial is based on a predefined scene located in /src/amira_belender_rendering/scenes/simpleobject.py We suggest to take a look at it and try to understand it. The file can be used as prototypical example in order to set up other scenes. For a more in depth explanation refer to the following tutorial on set up a simple custom sceneario Assuming you are familiar with python, a few explanations are in order: the class SimpleObjectConfiguration is used to manage scene specific config values the class SimpleObject is used to control the logic of the desired scene. More specifically, at runtime, after setting up a class object, blender calls the generate_dataset which, in turn, takes care of: randomizing the environment (background) textures randomizing the pose of the target object (in this case a metal cap) adjust the file paths through the rendermanager calling the rendering performing some post processing steps def generate_dataset ( self ): # filename setup image_count = self . config . dataset . image_count if image_count <= 0 : return False format_width = int ( ceil ( log ( image_count , 10 ))) i = 0 while i < self . config . dataset . image_count : # generate render filename: adhere to naming convention base_filename = f \"s { i : 0 { format_width }} _v0\" # randomize environment and object transform self . randomize_environment_texture () self . randomize_object_transforms () # setup render managers' path specification self . renderman . setup_pathspec ( self . dirinfo , base_filename , self . objs ) # render the image self . renderman . render () # try to postprocess. This might fail, in which case we should # attempt to re-render the scene with different randomization try : self . renderman . postprocess ( self . dirinfo , base_filename , bpy . context . scene . camera , self . objs , self . config . camera_info . zeroing , postprocess_config = self . config . postprocess ) except ValueError : self . logger . warn ( \"ValueError during post-processing, re-generating image index {i} \" ) else : i = i + 1 return True","title":"Scene"},{"location":"tutorials/renderpredefinedscene/#data","text":"Make sure the files B.ply and tool_cap_x10.ply are placed in /data/cad/parts . Also, as background textures we use images from Open Images a publicly available large dataset of various images. Make sure to download they images and store them locally, e.g., in a dedicated folder in $HOME/data/OpenImagesV4/Images . Note : In the tutorial we are going to render images of a metal cap with random background hence, we do not need any other data (such as a dedicated blender scene).","title":"Data"},{"location":"tutorials/renderpredefinedscene/#configuration-file","text":"Take a look at the configuration file /config/examples/single_object_toolcap_example.cfg . The file is used to specifiy a bunch of configuration values for the dataset we are going to render such as the number of images to render etc. As you can see, we exploits some enviroment variables in order to parametrize some configuration values and make them agnostic user specific settings. In particular, these are: OUTDIR : path to folder where the rendered dataset will be stored DATA : path where backgrounds data are stored DATA_GFX : path where graphics data are stored In order to successfully render the dataset, you need to explictly set the above three variables. You can do that e.g., in you ~/.bashrc or inline as shown in the following section.","title":"Configuration file"},{"location":"tutorials/renderpredefinedscene/#commence-the-rendering","text":"Now all that is left is to submit the rendering. It is assumed you have amira_blender_rendering repo available in your $HOME and that you set up the data as explained above . Then, to commence the rendering run OUTDIR = ~/data \\ DATA_GFX = ~/amira_blender_rendering/data \\ DATA = ~/data \\ abrgen --config ~/amira_blender_rendering/config/examples/single_object_toolcap_example.cfg In case you get Error: Could not import amira_blender_rendering. Either install it as a package, or specify a valid path to its location with the --abr-path command line argument. specify the --abr-path CLI argument as explained here .","title":"Commence the rendering "},{"location":"tutorials/simplecustomscenario/","text":"Set Up a Simple Custom Scenario \u00b6 ABR allows you to setup and render your own scenario. For further details and to take inspiration refer to the existing scenarios located in src/amira_blender_rendering/scenes/ In order to setup your custom scenario few necessary requirements must be met. custom classes discover scenes NOTE Adding custom scenes requires ABR to be installed as editable (see installation ). Create the configuration and scene classes \u00b6 Start by creating a new .py file (e.g. mycoolscenario.py ) in src/amira_blender_rendering/scenes . You could start from a minimal set of imports such as # necessary imports import bpy from amira_blender_rendering.utils import camera as camera_utils from amira_blender_rendering.utils.io import expandpath import amira_blender_rendering.scenes as abr_scenes import amira_blender_rendering.interfaces as interfaces # useful imports import os import pathlib from math import ceil , log import random import numpy as np from amira_blender_rendering.utils.logging import get_logger from amira_blender_rendering.dataset import get_environment_textures , build_directory_info , dump_config import amira_blender_rendering.utils.blender as blnd import amira_blender_rendering.nodes as abr_nodes import amira_blender_rendering.math.geometry as abr_geom As standard in the scenarios already provided, the most natural step is to add a custom configuration class # for later convenience, give your scene/scenario a name _scene_name = 'MyCoolScenario' # Custom configuration class. # Inheriting from the BaseConfiguration which defines dataset, # camera and render -specific configuration parameters. # abr_scene.register(name=_scene_name, type='config') class MyCoolScenarioConfiguration ( abr_scenes . BaseConfiguration ): def __init__ ( self ): super ( MyCoolScenarioConfiguration , self ) . __init__ () # Add all the scenario-specific configuration parameters, # see the configuration documentation for an overview. # This will be accessible in the config file or from the command line. self . add_param ( ... The following and most important thing is to add your custom scenario class # Custom scenario class. # Inheriting from ABRScene which defines an interface (in the form of # an abstract base class) for all ABR scenes. # This class should be expanded and modified at user wish to implement # all the functionalities needed to interact with her/his custom scenario. # E.g., object identification, random pose initialization etc. @abr_scenes . register ( name = _scene_name , type = 'scene' ) class MyCoolScenario ( interfaces . ABRScene ): \"\"\" Example class implementing a custom user-defined scenario \"\"\" def __init__ ( self , ** kwargs ): super ( MyCoolScenario , self ) . __init__ () # [Optional] some logging capabilities self . logger = get_logger () # [Recommended] get the configuration, if one was passed in self . config = kwargs . get ( 'config' , MyCoolScenarioConfiguration ()) # [Mandatory] make use of ABR RenderManager for interaction with Blender self . renderman = abr_scenes . RenderManager () # [Optional] you might have to post-process the configuration self . postprocess_config () # [Recommended] set up directories information, e.g., for multiple cameras self . setup_dirinfo () # [Mandatory] set up anything that we need for the scene before doing anything else. # For instance, removing all default objects self . setup_scene () # [Mandatory] after the scene, let's set up the render manager self . renderman . setup_renderer ( self . config . render_setup . integrator , self . config . render_setup . denoising , self . config . render_setup . samples , self . config . render_setup . motion_blur ) # [Recommended] setup environment texture information # This could be as simple as importing a list of all the available textures. # In our case we often use images from OpenImagesV4 as textures for light reflection. # In your case this could be a single image or something more sophisticated. self . environment_textures = get_environment_textures ( self . config . scene_setup . environment_textures ) # [Mandatory] setup the camera that we wish to use self . setup_cameras () # [Mandatory] setup render / output settings self . setup_render_output () # [Mandatory] setup the object that we want to render self . setup_objects () # [Mandatory] finally, let's setup the compositor # by passing it the list of defined objects, see setup_objects. self . renderman . setup_compositor ( self . objects ) \"\"\" [Mandatory] You need to implement the abstract methods \"\"\" def dump_config ( self ): \"\"\" Dump dataset configuration into corresponding directory for documentation \"\"\" # Depending if you are rendering images of a single dataset from a single camera... pathlib . Path ( self . dirinfo . base_path ) . mkdir ( parents = True , exist_ok = True ) dump_config ( self . config , self . dirinfo . base_path ) # ...or multiple dataset from multiple cameras # See setup_dirinfo() for dirinfo in self . dirinfos : output_path = dirinfo . base_path pathlib . Path ( output_path ) . mkdir ( parents = True , exist_ok = True ) dump_config ( self . config , output_path ) def generate_dataset ( self ): \"\"\" Main method used to iterate over and generate the dataset The method should implement all the necessary computations to manipulate existing objects, call to render and postprocessing At generation time, this is called by the main script abrgen, which, in turn, calls cli/render_dataset.py \"\"\" # Please refer to abr/scenes/simpletoolcap.py or abr/scenes/workspacescenario.py # for code-specific implementation. # The method could require implementation of additional supporting ones as for instance # - visibility tests # - object pose randomization # - phisics forward simulation Recall, that the above code is only meant to hint your own one. After the constructor, you are left with implementing the class methods. In the following we provide some examples. # [Recommended] Set up directories information, e.g., for multiple cameras def setup_dirinfo ( self ): # This could be a single line of code such as self . dirinfo = build_directory_info ( self . config . dataset . base_path ) #.. as well as a list of multiple dictionaries depending if you have # one or multiple cameras self . dirinfos = list () for cam in self . config . scene_setup . cameras : camera_base_path = f \" { self . config . dataset . base_path } - { cam } \" dirinfo = build_directory_info ( camera_base_path ) self . dirinfos . append ( dirinfo ) # [Mandatory] set up anything that we need for the scene before doing anything else. # For instance, removing all default objects def setup_scene ( self ): # This highly depends on your scene. \"\"\"[For more complicated scene which use blender modeling]\"\"\" # You might just want to load the blender file where you previously modeled your scene... bpy . ops . wm . open_mainfile ( filepath = expandpath ( self . config . scene_setup . blend_file )) # ...plus some additional operation such as self . logger . info ( \"Hiding all dropzones from viewport\" ) bpy . data . collections [ 'Dropzones' ] . hide_viewport = True \"\"\"[For simple rendering of objects]\"\"\" # Conversely, if you do not really have an explicit scene you might # first, want to delete everything (just to be sure)... blnd . clear_all_objects () # ... After you could setup lighting. self . lighting = abr_scenes . ThreePointLighting () # [Mandatory] setup the camera that we wish to use def setup_cameras () # This highly depends on your scene and the cameras that are setup in the blender file # We recommend to take a look at abr/scenes/simpletoolcap.py or abr/scenes/workstationscenarios.py # The general workflow is: # - get the intrinsic from the config and convert them into suitable format, e.g., K matrix # - select each existing camera and set its intrinsic values # [Mandatory] setup render / output settings def setup_render_output () # This mainly serves to set up the render dimension if ( self . config . camera_info . width > 0 ) and ( self . config . camera_info . height > 0 ): bpy . context . scene . render . resolution_x = self . config . camera_info . width bpy . context . scene . render . resolution_y = self . config . camera_info . height # In addition you might want to include additional custom operations... # [Mandatory] setup the object that we want to render self . setup_objects () \"\"\" Setup all objects of interest to control in the scene. The main purpose is to create a list of objects (dict) such as each object is a dictionary with the following structure obj = { 'id_mask' (str) : '', 'model_name' (str) : obj_type, 'model_id' (int) : model_id, 'object_id' (int) : j, 'bpy' (bpy.obj) : bpy (blender) obj }) \"\"\" # For code-specific implementation please refer to # abr/scenes/simpletoolcap.py and/or abr/scenes/workspacescenarios.py Make the custom scene discoverable \u00b6 As you might have noticed, right before the class definition for your custom scene and its corresponding configurations, we used the following syntax _scene_name = 'MyCoolScenario' @abr_scene . register ( name = _scene_name , type = 'config' ) class MyCoolScenarioConfiguration ( abr_scenes . BaseConfiguration ): def __init__ ( self ): # additional code @abr_scenes . register ( name = _scene_name , type = 'scene' ) class MyCoolScenario ( interfaces . ABRScene ): def __init__ ( self , ** kwargs ): # additional code In particular, the line @abr_scene.register(name=, type=) might appear a bit obscure if you are not familiar with python. In any case, it is important to add this line to your custom code. Internally, it is used to automatically register your scene (and its configuration), expose and make it available to ABR. If you do not add those lines and you try to run abrgen with your brand new scene you most likely are going to encounter a RuntimeError: Invalid configuration: Unknown scene_type MyCoolSceneario NOTE In the config file used at rendering time, you need to use the value set in _scene_name to correctly select your custom scenario.","title":"Intermediate - set up a simple custom scenario"},{"location":"tutorials/simplecustomscenario/#set-up-a-simple-custom-scenario","text":"ABR allows you to setup and render your own scenario. For further details and to take inspiration refer to the existing scenarios located in src/amira_blender_rendering/scenes/ In order to setup your custom scenario few necessary requirements must be met. custom classes discover scenes NOTE Adding custom scenes requires ABR to be installed as editable (see installation ).","title":"Set Up a Simple Custom Scenario"},{"location":"tutorials/simplecustomscenario/#create-the-configuration-and-scene-classes","text":"Start by creating a new .py file (e.g. mycoolscenario.py ) in src/amira_blender_rendering/scenes . You could start from a minimal set of imports such as # necessary imports import bpy from amira_blender_rendering.utils import camera as camera_utils from amira_blender_rendering.utils.io import expandpath import amira_blender_rendering.scenes as abr_scenes import amira_blender_rendering.interfaces as interfaces # useful imports import os import pathlib from math import ceil , log import random import numpy as np from amira_blender_rendering.utils.logging import get_logger from amira_blender_rendering.dataset import get_environment_textures , build_directory_info , dump_config import amira_blender_rendering.utils.blender as blnd import amira_blender_rendering.nodes as abr_nodes import amira_blender_rendering.math.geometry as abr_geom As standard in the scenarios already provided, the most natural step is to add a custom configuration class # for later convenience, give your scene/scenario a name _scene_name = 'MyCoolScenario' # Custom configuration class. # Inheriting from the BaseConfiguration which defines dataset, # camera and render -specific configuration parameters. # abr_scene.register(name=_scene_name, type='config') class MyCoolScenarioConfiguration ( abr_scenes . BaseConfiguration ): def __init__ ( self ): super ( MyCoolScenarioConfiguration , self ) . __init__ () # Add all the scenario-specific configuration parameters, # see the configuration documentation for an overview. # This will be accessible in the config file or from the command line. self . add_param ( ... The following and most important thing is to add your custom scenario class # Custom scenario class. # Inheriting from ABRScene which defines an interface (in the form of # an abstract base class) for all ABR scenes. # This class should be expanded and modified at user wish to implement # all the functionalities needed to interact with her/his custom scenario. # E.g., object identification, random pose initialization etc. @abr_scenes . register ( name = _scene_name , type = 'scene' ) class MyCoolScenario ( interfaces . ABRScene ): \"\"\" Example class implementing a custom user-defined scenario \"\"\" def __init__ ( self , ** kwargs ): super ( MyCoolScenario , self ) . __init__ () # [Optional] some logging capabilities self . logger = get_logger () # [Recommended] get the configuration, if one was passed in self . config = kwargs . get ( 'config' , MyCoolScenarioConfiguration ()) # [Mandatory] make use of ABR RenderManager for interaction with Blender self . renderman = abr_scenes . RenderManager () # [Optional] you might have to post-process the configuration self . postprocess_config () # [Recommended] set up directories information, e.g., for multiple cameras self . setup_dirinfo () # [Mandatory] set up anything that we need for the scene before doing anything else. # For instance, removing all default objects self . setup_scene () # [Mandatory] after the scene, let's set up the render manager self . renderman . setup_renderer ( self . config . render_setup . integrator , self . config . render_setup . denoising , self . config . render_setup . samples , self . config . render_setup . motion_blur ) # [Recommended] setup environment texture information # This could be as simple as importing a list of all the available textures. # In our case we often use images from OpenImagesV4 as textures for light reflection. # In your case this could be a single image or something more sophisticated. self . environment_textures = get_environment_textures ( self . config . scene_setup . environment_textures ) # [Mandatory] setup the camera that we wish to use self . setup_cameras () # [Mandatory] setup render / output settings self . setup_render_output () # [Mandatory] setup the object that we want to render self . setup_objects () # [Mandatory] finally, let's setup the compositor # by passing it the list of defined objects, see setup_objects. self . renderman . setup_compositor ( self . objects ) \"\"\" [Mandatory] You need to implement the abstract methods \"\"\" def dump_config ( self ): \"\"\" Dump dataset configuration into corresponding directory for documentation \"\"\" # Depending if you are rendering images of a single dataset from a single camera... pathlib . Path ( self . dirinfo . base_path ) . mkdir ( parents = True , exist_ok = True ) dump_config ( self . config , self . dirinfo . base_path ) # ...or multiple dataset from multiple cameras # See setup_dirinfo() for dirinfo in self . dirinfos : output_path = dirinfo . base_path pathlib . Path ( output_path ) . mkdir ( parents = True , exist_ok = True ) dump_config ( self . config , output_path ) def generate_dataset ( self ): \"\"\" Main method used to iterate over and generate the dataset The method should implement all the necessary computations to manipulate existing objects, call to render and postprocessing At generation time, this is called by the main script abrgen, which, in turn, calls cli/render_dataset.py \"\"\" # Please refer to abr/scenes/simpletoolcap.py or abr/scenes/workspacescenario.py # for code-specific implementation. # The method could require implementation of additional supporting ones as for instance # - visibility tests # - object pose randomization # - phisics forward simulation Recall, that the above code is only meant to hint your own one. After the constructor, you are left with implementing the class methods. In the following we provide some examples. # [Recommended] Set up directories information, e.g., for multiple cameras def setup_dirinfo ( self ): # This could be a single line of code such as self . dirinfo = build_directory_info ( self . config . dataset . base_path ) #.. as well as a list of multiple dictionaries depending if you have # one or multiple cameras self . dirinfos = list () for cam in self . config . scene_setup . cameras : camera_base_path = f \" { self . config . dataset . base_path } - { cam } \" dirinfo = build_directory_info ( camera_base_path ) self . dirinfos . append ( dirinfo ) # [Mandatory] set up anything that we need for the scene before doing anything else. # For instance, removing all default objects def setup_scene ( self ): # This highly depends on your scene. \"\"\"[For more complicated scene which use blender modeling]\"\"\" # You might just want to load the blender file where you previously modeled your scene... bpy . ops . wm . open_mainfile ( filepath = expandpath ( self . config . scene_setup . blend_file )) # ...plus some additional operation such as self . logger . info ( \"Hiding all dropzones from viewport\" ) bpy . data . collections [ 'Dropzones' ] . hide_viewport = True \"\"\"[For simple rendering of objects]\"\"\" # Conversely, if you do not really have an explicit scene you might # first, want to delete everything (just to be sure)... blnd . clear_all_objects () # ... After you could setup lighting. self . lighting = abr_scenes . ThreePointLighting () # [Mandatory] setup the camera that we wish to use def setup_cameras () # This highly depends on your scene and the cameras that are setup in the blender file # We recommend to take a look at abr/scenes/simpletoolcap.py or abr/scenes/workstationscenarios.py # The general workflow is: # - get the intrinsic from the config and convert them into suitable format, e.g., K matrix # - select each existing camera and set its intrinsic values # [Mandatory] setup render / output settings def setup_render_output () # This mainly serves to set up the render dimension if ( self . config . camera_info . width > 0 ) and ( self . config . camera_info . height > 0 ): bpy . context . scene . render . resolution_x = self . config . camera_info . width bpy . context . scene . render . resolution_y = self . config . camera_info . height # In addition you might want to include additional custom operations... # [Mandatory] setup the object that we want to render self . setup_objects () \"\"\" Setup all objects of interest to control in the scene. The main purpose is to create a list of objects (dict) such as each object is a dictionary with the following structure obj = { 'id_mask' (str) : '', 'model_name' (str) : obj_type, 'model_id' (int) : model_id, 'object_id' (int) : j, 'bpy' (bpy.obj) : bpy (blender) obj }) \"\"\" # For code-specific implementation please refer to # abr/scenes/simpletoolcap.py and/or abr/scenes/workspacescenarios.py","title":"Create the configuration and scene classes"},{"location":"tutorials/simplecustomscenario/#make-the-custom-scene-discoverable","text":"As you might have noticed, right before the class definition for your custom scene and its corresponding configurations, we used the following syntax _scene_name = 'MyCoolScenario' @abr_scene . register ( name = _scene_name , type = 'config' ) class MyCoolScenarioConfiguration ( abr_scenes . BaseConfiguration ): def __init__ ( self ): # additional code @abr_scenes . register ( name = _scene_name , type = 'scene' ) class MyCoolScenario ( interfaces . ABRScene ): def __init__ ( self , ** kwargs ): # additional code In particular, the line @abr_scene.register(name=, type=) might appear a bit obscure if you are not familiar with python. In any case, it is important to add this line to your custom code. Internally, it is used to automatically register your scene (and its configuration), expose and make it available to ABR. If you do not add those lines and you try to run abrgen with your brand new scene you most likely are going to encounter a RuntimeError: Invalid configuration: Unknown scene_type MyCoolSceneario NOTE In the config file used at rendering time, you need to use the value set in _scene_name to correctly select your custom scenario.","title":"Make the custom scene discoverable"}]}